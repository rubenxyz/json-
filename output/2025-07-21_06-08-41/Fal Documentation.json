{
  "instructions": {
    "format": "json",
    "version": "2.0",
    "generated_at": "2025-07-21T06:08:41.860911",
    "source": "markdown",
    "total_instructions": 246
  },
  "folders": [
    {
      "id": "01",
      "name": "Fal Documentation",
      "order": 1,
      "files": [
        {
          "id": "01.01",
          "name": "01.1 Introduction  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.1 Introduction  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "1",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "01.1 Introduction  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "2",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "01.1 Introduction  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "3",
                      "type": "subsubsection",
                      "title": "Key features:",
                      "content": "\n-   Ready-to-use AI inference APIs optimized for speed and scalability\n-   Serverless deployment options for custom AI models\n-   Interactive UI playgrounds for model experimentation\n-   Specialized in image generation through the Flux API\n-   Enterprise features including private models and preference fine-tuning capabilities\n\nThe platform aims to provide developers with the fastest and most reliable infrastructure for integrating AI-powered media generation into their applications, with a focus on real-time user experiences.",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "01.1 Introduction  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.02",
          "name": "Fastest FLUX Endpoint  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.11 Fastest FLUX/Fastest FLUX Endpoint  fal.ai Docs  fal.ai Docs.md",
          "sections": []
        },
        {
          "id": "01.03",
          "name": "FAQ  fal.ai Documentation  fal.ai Docs.md",
          "path": "01. fal/01.12 FAQ/FAQ  fal.ai Documentation  fal.ai Docs.md",
          "sections": [
            {
              "id": "4",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "5",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "6",
                      "type": "subsubsection",
                      "title": "When logging-in with GitHub I am asked for a one-time code that I never receive in my email",
                      "content": "\nLogging with GitHub means that the one-time code It’s being sent to the primary email in your GitHub account.\n\nYou may have created your GitHub account with an email you no longer monitor, so check [their documentation](https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-personal-account-on-github/managing-email-preferences/changing-your-primary-email-address) on how to find out which one it is set as and change it if appropriate.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "7",
                      "type": "subsubsection",
                      "title": "What is the retention policy for the files generated by fal.ai?",
                      "content": "\nThe files generated by fal.ai are guaranteed to be available for at least **7 days**. After that, they may be deleted at any time. We recommend that you download and store on your own storage any files that you want to keep for longer.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "8",
                      "type": "subsubsection",
                      "title": "Can I use the generated files for commercial purposes?",
                      "content": "\nEach model has its own license. Most of the endpoints available at fal are available for commercial use. Check for the label on each model page:\n\n-   Commercial use : Commercial use is allowed. Even when the underlying model is not open-source, if it’s marked with this badge it means that fal has the necessary rights to provide the service for commercial use.\n-   Research only : This model is available for research purposes only. You can use the API to generate images for research purposes, but you cannot use them for commercial purposes.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "9",
                      "type": "subsubsection",
                      "title": "What is the Partner API?",
                      "content": "\n-   Partner API : Partner APIs are hosted by our partners. Therefore, we cannot offer percentage discount on them and cannot guarantee their availability.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "10",
                      "type": "subsubsection",
                      "title": "Is there a rate limit?",
                      "content": "\nThe rate limit for the API is **10 concurrent tasks** per user, across all endpoints. For enterprise customers, we can scale this up, [contact us](mailto:support@fal.ai) if you need more rate limits.\n\nNote that we reserve the right to prioritize API requests over requests made through our Playground UI.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "11",
                      "type": "subsubsection",
                      "title": "Do you charge for failed requests?",
                      "content": "\nFailures originated from our side, such as server errors or any HTTP status 5xx, are not charged. However, if the failure is due to an error in the request, such as an invalid input, which can result in HTTP status 422, the request will be charged.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "12",
                      "type": "subsubsection",
                      "title": "Do my credits expire?",
                      "content": "\nYes, the credits you purchase expire in 365 days. Free credits or credits from coupons expire in 90 days.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "13",
                      "type": "subsubsection",
                      "title": "Can I switch to an invoice-based payment?",
                      "content": "\nYes, we offer invoice-based payments for customers with higher volumes. Please [contact us](mailto:support@fal.ai) with information about your expected load.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "14",
                      "type": "subsubsection",
                      "title": "Do I pay for cold starts?",
                      "content": "\nNo, although cold start for our main endpoints is very rare, you will not be charged for them when they happen.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "15",
                      "type": "subsubsection",
                      "title": "Can I deploy my own models?",
                      "content": "\nIf you want access to deploy a model or app for your private use, please [contact us](mailto:support@fal.ai).\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "16",
                      "type": "subsubsection",
                      "title": "Do you offer discounts?",
                      "content": "\nYes, we offer discounts for customers with higher volumes. Please [contact us](mailto:support@fal.ai) with information about your expected load.",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "FAQ  fal.ai Documentation  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.04",
          "name": "Error Reference  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.13 Errors/Error Reference  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "17",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Error Reference  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "18",
                  "type": "subsection",
                  "title": "Error Response",
                  "content": "\nWhen an API request fails due to client-side input issues (like validation errors) or server-side problems, the API returns a structured error response. This response includes a standard HTTP status code, specific headers, and a JSON body detailing the errors.\n",
                  "parent_section": "Main Content",
                  "file": "Error Reference  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "19",
                      "type": "subsubsection",
                      "title": "Error Response Structure",
                      "content": "\nThe error response consists of:\n\n1.  **HTTP Status Code:** Indicates the general category of the error (e.g., `500` for internal errors, `504` for timeouts).\n2.  **Headers:** Includes required headers like `X-Fal-Retryable`.\n3.  **JSON Body:** Contains a `detail` field which is an array of `Error` objects.\n\n| Header | Description |\n| --- | --- |\n| `X-Fal-Retryable` | **\\[OPTIONAL\\]** A boolean (`\"true\"` or `\"false\"`) indicating if retrying the _exact same_ request might succeed (e.g., transient issues). |\n",
                      "parent_subsection": "Error Response",
                      "parent_section": "Main Content",
                      "file": "Error Reference  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "20",
                      "type": "subsubsection",
                      "title": "Error Object Structure",
                      "content": "\nThe `detail` field is an array where each object represents a specific error.\n\n| Property | Description |\n| --- | --- |\n| `loc` | **\\[REQUIRED\\]** An array indicating the location of the error (e.g., `[\"body\", \"field_name\"]` for input validation, `[\"body\"]` for general errors). The first item in the loc list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error. |\n| `msg` | **\\[REQUIRED\\]** A human-readable description of the error. **Client code should not parse and rely on the msg field.** |\n| `type` | **\\[REQUIRED\\]** A unique, **machine-readable** string identifying the error category (e.g., `image_too_large`). Use this for conditional logic. |\n| `url` | **\\[REQUIRED\\]** A link to documentation about this specific error `type` (e.g., [https://docs.fal.ai/errors/#image\\_too\\_large\\`](https://docs.fal.ai/errors/#image_too_large%60)). Primarily for developers. |\n| `ctx` | **\\[OPTIONAL\\]** An object with additional structured, **machine-readable** context for the error `type` (e.g., `{\"max_height\": 1024, \"max_width\": 1024}` for `image_too_large`). |\n| `input` | **\\[OPTIONAL\\]** The input that caused the error. |\n",
                      "parent_subsection": "Error Response",
                      "parent_section": "Main Content",
                      "file": "Error Reference  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "21",
                  "type": "subsection",
                  "title": "Error Types",
                  "content": "\nThis error indicates that the provided video file is shorter than the minimum required duration.\n\n-   **Status Code:** 422\n-   **Retryable:** `false`.\n-   **Context (`ctx`):**\n    -   `min_duration`: The minimum required duration in seconds.\n    -   `provided_duration`: The duration of the provided video file in seconds.\n\n```\n[ { \"loc\": [\"body\", \"video_file\"], \"msg\": \"Video duration is too short. Minimum is 3 seconds, provided is 1 seconds.\", \"type\": \"video_duration_too_short\", \"url\": \"https://docs.fal.ai/errors/#video_duration_too_short\", \"ctx\": { \"min_duration\": 3, \"provided_duration\": 1 }, \"input\": \"https://example.com/short_video.mp4\" }]\n```",
                  "parent_section": "Main Content",
                  "file": "Error Reference  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                }
              ]
            }
          ]
        },
        {
          "id": "01.05",
          "name": "01.2 Quickstart with fal  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.2 Quickstart with fal  fal.ai Docs  fal.ai Docs.md",
          "sections": []
        },
        {
          "id": "01.06",
          "name": "Generate Images from Text Tutorial  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.3 Guides/Generate Images from Text Tutorial  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "22",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Generate Images from Text Tutorial  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "23",
                  "type": "subsection",
                  "title": "How to Generate Images using the fal API",
                  "content": "\nTo generate images using the fal API, you need to send a request to the appropriate endpoint with the desired input parameters. The API uses pre-trained models to generate images based on the provided text prompt. This allows you to create images by simply describing what you want in natural language.\n\nHere’s an example of how to generate an image using the fal API from text:\n\n```\nimport { fal } from \"@fal-ai/client\";const result = await fal.subscribe(\"fal-ai/flux/dev\", { input: { prompt: \"a face of a cute puppy, in the style of pixar animation\", },})\n;\n```\n",
                  "parent_section": "Main Content",
                  "file": "Generate Images from Text Tutorial  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "24",
                  "type": "subsection",
                  "title": "How to select the model to use",
                  "content": "\nfal offers a variety of image generation models. You can select the model that best fits your needs based on the style and quality of the images you want to generate. Here are some of the available models:\n\n-   [fal-ai/flux/dev](https://fal.ai/models/fal-ai/flux/dev): FLUX.1 \\[dev\\] is a 12 billion parameter flow transformer that generates high-quality images from text. It is suitable for personal and commercial use.\n-   [fal-ai/recraft-v3](https://fal.ai/models/fal-ai/recraft-v3): Recraft V3 is a text-to-image model with the ability to generate long texts, vector art, images in brand style, and much more. As of today, it is SOTA in image generation, proven by Hugging Face’s industry-leading Text-to-Image Benchmark by Artificial Analysis.\n-   [fal-ai/stable-diffusion-v35-large](https://fal.ai/models/fal-ai/stable-diffusion-v35-large): Stable Diffusion 3.5 Large is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\n\nTo select a model, simply specify the model ID in the subscribe method as shown in the example above. You can find more models and their descriptions in the [Text to Image Models](https://fal.ai/models?categories=text-to-image) page.",
                  "parent_section": "Main Content",
                  "file": "Generate Images from Text Tutorial  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                }
              ]
            }
          ]
        },
        {
          "id": "01.07",
          "name": "Generate Videos from Image Tutorial  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.3 Guides/Generate Videos from Image Tutorial  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "25",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Generate Videos from Image Tutorial  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "26",
                  "type": "subsection",
                  "title": "How to Generate Videos using the fal API",
                  "content": "\nfal offers a simple and easy-to-use API that allows you to generate videos from your images using pre-trained models. This endpoint is perfect for creating video clips from your images for various use cases such as social media, marketing, and more.\n\nHere is an example of how to generate videos using the fal API:\n\n```\nimport { fal } from \"@fal-ai/client\";const result = await fal.subscribe(\"fal-ai/minimax-video/image-to-video\", { input: { prompt: \"A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.\", image_url: \"https://fal.media/files/elephant/8kkhB12hEZI2kkbU8pZPA_test.jpeg\" },})\n;\n```\n",
                  "parent_section": "Main Content",
                  "file": "Generate Videos from Image Tutorial  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "27",
                  "type": "subsection",
                  "title": "How to select the model to use",
                  "content": "\nfal offers a variety of video generation models. You can select the model that best fits your needs based on the style and quality of the images you want to generate. Here are some of the available models:\n\n-   [fal-ai/minimax-video](https://fal.ai/models/fal-ai/minimax-video/image-to-video): Generate video clips from your images using MiniMax Video model.\n-   [fal-ai/luma-dream-machine](https://fal.ai/models/fal-ai/luma-dream-machine/image-to-video): Generate video clips from your images using Luma Dream Machine v1.5\n-   [fal-ai/kling-video/v1/standard](https://fal.ai/models/fal-ai/kling-video/v1/standard/image-to-video): Generate video clips from your images using Kling 1.0\n\nTo select a model, simply specify the model ID in the subscribe method as shown in the example above. You can find more models and their descriptions in the [Image to Video Models](https://fal.ai/models?categories=image-to-video) page.",
                  "parent_section": "Main Content",
                  "file": "Generate Videos from Image Tutorial  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                }
              ]
            }
          ]
        },
        {
          "id": "01.08",
          "name": "Use LLMs Tutorial  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.3 Guides/Use LLMs Tutorial  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "28",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Use LLMs Tutorial  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "29",
                  "type": "subsection",
                  "title": "How to select LLM model to use",
                  "content": "\nfal offers a variety of LLM models. You can select the model that best fits your needs based on the style and quality of the text you want to generate. Here are some of the available models:\n\n-   `anthropic/claude-3.5-sonnet`: Claude 3.5 Sonnet\n-   `google/gemini-pro-1.5`: Gemini Pro 1.5\n-   `meta-llama/llama-3.2-3b-instruct`: Llama 3.2 3B Instruct\n-   `openai/gpt-4o`: GPT-4o\n\nTo select a model, simply specify the model ID in the `model` field as shown in the example above. You can find more LLMs in the [Any LLM](https://fal.ai/models/fal-ai/any-llm) page.",
                  "parent_section": "Main Content",
                  "file": "Use LLMs Tutorial  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                }
              ]
            }
          ]
        },
        {
          "id": "01.09",
          "name": "GitHub Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.4 Authentication/GitHub Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "30",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "GitHub Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "31",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "GitHub Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "32",
                      "type": "subsubsection",
                      "title": "Logging in",
                      "content": "\n[Installing fal](https://docs.fal.ai/quick-start) Python library lets you use the `fal` CLI, which you can use to authenticate. In your terminal, you can run the following command:\n\nFollow the instructions on your terminal to confirm your credentials. Once you’re done, you should get a success message in your terminal.\n\nNow you’re ready to write your first fal function!\n\n**Note:** Your login credentials are persisted on your local machine and cannot be transferred to another machine. If you want to use fal in your CI/CD, you will need to use [key-based credentials](https://docs.fal.ai/authentication/key-based).",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "GitHub Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.10",
          "name": "Key-Based Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.4 Authentication/Key-Based Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "33",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Key-Based Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "34",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Key-Based Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "35",
                      "type": "subsubsection",
                      "title": "Generating the keys",
                      "content": "\nNavigate to our dashboard keys page and generate a key from the UI [fal.ai/dashboard/keys](https://fal.ai/dashboard/keys)\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Key-Based Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "36",
                      "type": "subsubsection",
                      "title": "Scopes",
                      "content": "\n-   Grants full access to private models.\n-   Grants full access to CLI operations.\n-   Grants access to ready-to-use models.",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Key-Based Authentication Authentication  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.11",
          "name": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.5 Client Libraries/Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "37",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "38",
                  "type": "subsection",
                  "title": "Introduction",
                  "content": "\nThe client for Python provides a seamless interface to interact with fal.\n",
                  "parent_section": "Main Content",
                  "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "39",
                  "type": "subsection",
                  "title": "Installation",
                  "content": "\nFirst, add the client as a dependency in your project:\n",
                  "parent_section": "Main Content",
                  "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "40",
                  "type": "subsection",
                  "title": "Features",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "41",
                      "type": "subsubsection",
                      "title": "1\\. Call an endpoint",
                      "content": "\nEndpoints requests are managed by a queue system. This allows fal to provide a reliable and scalable service.\n\nThe `subscribe` method allows you to submit a request to the queue and wait for the result.\n\n-   [Python](https://docs.fal.ai/clients/python#tab-panel-8)\n-   [Python (async)](https://docs.fal.ai/clients/python#tab-panel-9)\n\n```\nimport fal_clientdef\non_queue_update(update)\n: if isinstance(update, fal_client.InProgress)\n: for log in update.logs: print(log[\"message\"])\nresult = fal_client.subscribe( \"fal-ai/flux/dev\", arguments={ \"prompt\": \"a cat\", \"seed\": 6252023, \"image_size\": \"landscape_4_3\", \"num_images\": 4 }, with_logs=True, on_queue_update=on_queue_update,)\nprint(result)\n```\n",
                      "parent_subsection": "Features",
                      "parent_section": "Main Content",
                      "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "42",
                      "type": "subsubsection",
                      "title": "2\\. Queue Management",
                      "content": "\nGet the result of a specific request from the queue:\n\n-   [Python](https://docs.fal.ai/clients/python#tab-panel-14)\n-   [Python (async)](https://docs.fal.ai/clients/python#tab-panel-15)\n\n```\nresult = fal_client.result(\"fal-ai/flux/dev\", request_id)\n```\n",
                      "parent_subsection": "Features",
                      "parent_section": "Main Content",
                      "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "43",
                      "type": "subsubsection",
                      "title": "3\\. File Uploads",
                      "content": "\nSome endpoints require files as input. However, since the endpoints run asynchronously, processed by the queue, you will need to provide URLs to the files instead of the actual file content.\n\nLuckily, the client library provides a way to upload files to the server and get a URL to use in the request.\n\n-   [Python](https://docs.fal.ai/clients/python#tab-panel-16)\n-   [Python (async)](https://docs.fal.ai/clients/python#tab-panel-17)\n\n```\nurl = fal_client.upload_file(\"path/to/file\")\n```\n",
                      "parent_subsection": "Features",
                      "parent_section": "Main Content",
                      "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "44",
                      "type": "subsubsection",
                      "title": "4\\. Streaming",
                      "content": "\nSome endpoints support streaming:\n\n-   [Python](https://docs.fal.ai/clients/python#tab-panel-18)\n-   [Python (async)](https://docs.fal.ai/clients/python#tab-panel-19)\n\n```\nimport fal_clientdef\nstream()\n: stream = fal_client.stream( \"fal-ai/flux/dev\", arguments={ \"prompt\": \"a cat\", \"seed\": 6252023, \"image_size\": \"landscape_4_3\", \"num_images\": 4 }, ) \nfor event in stream: print(event)\nif __name__ == \"__main__\": stream()\n```\n",
                      "parent_subsection": "Features",
                      "parent_section": "Main Content",
                      "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "45",
                      "type": "subsubsection",
                      "title": "5\\. Realtime Communication",
                      "content": "\nFor the endpoints that support real-time inference via WebSockets, you can use the realtime client that abstracts the WebSocket connection, re-connection, serialization, and provides a simple interface to interact with the endpoint:\n\n-   [Python](https://docs.fal.ai/clients/python#tab-panel-22)\n-   [Python (async)](https://docs.fal.ai/clients/python#tab-panel-23)\n",
                      "parent_subsection": "Features",
                      "parent_section": "Main Content",
                      "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "46",
                  "type": "subsection",
                  "title": "API Reference",
                  "content": "\nFor a complete list of available methods and their parameters, please refer to [Python API Reference documentation](https://fal-ai.github.io/fal/client).\n",
                  "parent_section": "Main Content",
                  "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "47",
                  "type": "subsection",
                  "title": "Support",
                  "content": "\nIf you encounter any issues or have questions, please visit the [GitHub repository](https://github.com/fal-ai/fal) or join our [Discord Community](https://discord.gg/fal-ai).",
                  "parent_section": "Main Content",
                  "file": "Client Library for Python Client Library  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                }
              ]
            }
          ]
        },
        {
          "id": "01.12",
          "name": "HTTP over WebSockets API  fal.ai Reference  fal.ai Docs.md",
          "path": "01. fal/01.6 Model Endpoints/HTTP over WebSockets API  fal.ai Reference  fal.ai Docs.md",
          "sections": [
            {
              "id": "48",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "HTTP over WebSockets API  fal.ai Reference  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": []
            },
            {
              "id": "49",
              "type": "section",
              "title": "they should be in order for i in range(3)",
              "content": ": import json response = json.loads(connection.recv()\n) \nprint(response)\n```\n\nAnd running this program would output:\n\n```\n{'output': '(Silence)\\n', 'partial': False, 'error': None}{'output': 'Growth\\n', 'partial': False, 'error': None}{'output': 'Personal fulfillment.\\n', 'partial': False, 'error': None}\n```\n",
              "file": "HTTP over WebSockets API  fal.ai Reference  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "50",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "they should be in order for i in range(3)",
                  "file": "HTTP over WebSockets API  fal.ai Reference  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "51",
                      "type": "subsubsection",
                      "title": "Example Program with Stream",
                      "content": "\nThe `fal-ai/any-llm/stream` model is a streaming model that can generate text in real-time. Here’s an example of how you can use it:\n\n```\nwith fal.apps.ws(\"fal-ai/any-llm/stream\") \nas connection: # NOTE: this app responds in 'text/event-stream' format # For example: # # event: event # data: {\"output\": \"Growth\",\n\"partial\": true,\n\"error\": null} for i in range(3)\n: connection.send( { \"model\": \"google/gemini-flash-1.5\", \"prompt\": f\"What is the meaning of life? Respond in {i+1} words.\", } ) \nfor i in range(3)\n: for bs in connection.stream()\n: lines = bs.decode()\n.replace(\"\\r\\n\", \"\\n\")\n.split(\"\\n\") \nevent = {} for line in lines: if not line: continue key,\nvalue = line.split(\":\", 1) \nevent[key] = value.strip() \nprint(event[\"data\"]) \nprint(\"----\")\n```\n\nAnd running this program would output:\n\n```\n{\"output\": \"Perspective\", \"partial\": true, \"error\": null}{\"output\": \"Perspective.\\n\", \"partial\": true, \"error\": null}{\"output\": \"Perspective.\\n\", \"partial\": true, \"error\": null}{\"output\": \"Perspective.\\n\", \"partial\": false, \"error\": null}----{\"output\": \"Find\", \"partial\": true, \"error\": null}{\"output\": \"Find meaning.\\n\", \"partial\": true, \"error\": null}{\"output\": \"Find meaning.\\n\", \"partial\": true, \"error\": null}{\"output\": \"Find meaning.\\n\", \"partial\": false, \"error\": null}----{\"output\": \"Be\", \"partial\": true, \"error\": null}{\"output\": \"Be, love, grow.\\n\", \"partial\": true, \"error\": null}{\"output\": \"Be, love, grow.\\n\", \"partial\": true, \"error\": null}{\"output\": \"Be, love, grow.\\n\", \"partial\": false, \"error\": null}----\n```",
                      "parent_subsection": "Subsection",
                      "parent_section": "they should be in order for i in range(3)",
                      "file": "HTTP over WebSockets API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.13",
          "name": "Model Endpoints API  fal.ai Reference  fal.ai Docs.md",
          "path": "01. fal/01.6 Model Endpoints/Model Endpoints API  fal.ai Reference  fal.ai Docs.md",
          "sections": []
        },
        {
          "id": "01.14",
          "name": "Queue API  fal.ai Reference  fal.ai Docs.md",
          "path": "01. fal/01.6 Model Endpoints/Queue API  fal.ai Reference  fal.ai Docs.md",
          "sections": [
            {
              "id": "52",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "53",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "54",
                      "type": "subsubsection",
                      "title": "Queue endpoints",
                      "content": "\nThe queue functionality is exposed via standardized per-model paths under `https://queue.fal.run`.\n\n| Endpoint | Method | Description |\n| --- | --- | --- |\n| **`https://queue.fal.run/{model_id}`** | POST | Adds a request to the queue for a top-level path |\n| **`https://queue.fal.run/{model_id}/{subpath}`** | POST | Adds a request to the queue for an optional subpath |\n| **`https://queue.fal.run/{model_id}/requests/{request_id}/status`** | GET | Gets the status of a request |\n| **`https://queue.fal.run/{model_id}/requests/{request_id}/status/stream`** | GET | Streams the status of a request until it’s completed |\n| **`https://queue.fal.run/{model_id}/requests/{request_id}`** | GET | Gets the response of a request |\n| **`https://queue.fal.run/{model_id}/requests/{request_id}/cancel`** | PUT | Cancels a request that has not started processing |\n\nParameters:\n\n-   `model_id`: the model ID consists of a namespace and model name separated by a slash, e.g. `fal-ai/fast-sdxl`. Many models expose only a single top-level endpoint, so you can directly call them by `model_id`.\n-   `subpath`: some models expose different capabilities at different sub-paths, e.g. `fal-ai/flux/dev`. The subpath (`/dev` in this case) should be used when making the request, but not when getting request status or results\n-   `request_id` is returned after adding a request to the queue. This is the identifier you use to check the status and get results and logs\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "55",
                      "type": "subsubsection",
                      "title": "Submit a request",
                      "content": "\nHere is an example of using curl to submit a request which will add it to the queue:\n\n```\ncurl -X POST https://queue.fal.run/fal-ai/fast-sdxl \\ -H \"Authorization: Key $FAL_KEY\" \\ -d '{\"prompt\": \"a cat\"}'\n```\n\nHere’s an example of a response with the `request_id`:\n\n```\n{ \"request_id\": \"80e732af-660e-45cd-bd63-580e4f2a94cc\", \"response_url\": \"https://queue.fal.run/fal-ai/fast-sdxl/requests/80e732af-660e-45cd-bd63-580e4f2a94cc\", \"status_url\": \"https://queue.fal.run/fal-ai/fast-sdxl/requests/80e732af-660e-45cd-bd63-580e4f2a94cc/status\", \"cancel_url\": \"https://queue.fal.run/fal-ai/fast-sdxl/requests/80e732af-660e-45cd-bd63-580e4f2a94cc/cancel\"}\n```\n\nThe payload helps you to keep track of your request with the `request_id`, and provides you with the necessary information to get the status of your request, cancel it or get the response once it’s ready, so you don’t have to build these endpoints yourself.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "56",
                      "type": "subsubsection",
                      "title": "Request status",
                      "content": "\nIf you want to keep track of the status of your request in real-time, you can use the streaming endpoint. The response is `text/event-stream` and each event is a JSON object with the status of the request exactly as the non-stream endpoint.\n\nThis endpoint will keep the connection open until the status of the request changes to `COMPLETED`.\n\nIt supports the same `logs` query parameter as the status.\n\n```\ncurl -X GET https://queue.fal.run/fal-ai/fast-sdxl/requests/{request_id}/status/stream\n```\n\nHere is an example of a stream of status updates:\n\n```\n$ curl https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/status/stream?logs=1 --header \"Authorization: Key $FAL_KEY\"data: {\"status\": \"IN_PROGRESS\",\n\"request_id\": \"3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"response_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"status_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/status\",\n\"cancel_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/cancel\",\n\"logs\": [],\n\"metrics\": {}}data: {\"status\": \"IN_PROGRESS\",\n\"request_id\": \"3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"response_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"status_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/status\",\n\"cancel_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/cancel\",\n\"logs\": [{\"timestamp\": \"2024-12-20T15:37:17.120314\",\n\"message\": \"INFO:TRYON:Preprocessing images...\",\n\"labels\": {}},\n{\"timestamp\": \"2024-12-20T15:37:17.286519\",\n\"message\": \"INFO:TRYON:Running try-on model...\",\n\"labels\": {}}],\n\"metrics\": {}}data: {\"status\": \"IN_PROGRESS\",\n\"request_id\": \"3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"response_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"status_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/status\",\n\"cancel_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/cancel\",\n\"logs\": [],\n\"metrics\": {}}: pingdata: {\"status\": \"IN_PROGRESS\",\n\"request_id\": \"3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"response_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"status_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/status\",\n\"cancel_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/cancel\",\n\"logs\": [],\n\"metrics\": {}}data: {\"status\": \"COMPLETED\",\n\"request_id\": \"3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"response_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf\",\n\"status_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/status\",\n\"cancel_url\": \"https://queue.fal.run/fashn/tryon/requests/3e3e5b55-45fb-4e5c-b4d1-05702dffc8bf/cancel\",\n\"logs\": [{\"timestamp\": \"2024-12-20T15:37:32.161184\",\n\"message\": \"INFO:TRYON:Finished running try-on model.\",\n\"labels\": {}}],\n\"metrics\": {\"inference_time\": 17.795265674591064}}\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "57",
                      "type": "subsubsection",
                      "title": "Cancelling a request",
                      "content": "\nIf your request has not started processing (status is `IN_QUEUE`), you may attempt to cancel it.\n\n```\ncurl -X PUT https://queue.fal.run/fal-ai/fast-sdxl/requests/{request_id}/cancel\n```\n\nIf the request has not already started processing, you will get a `202 Accepted` response with the following body:\n\n```\n{ \"status\": \"CANCELLATION_REQUESTED\"}\n```\n\nNote that a request may still be executed after getting this response if it was very late in the queue process.\n\nIf the request is already processed, you will get a `400 Bad Request` response with this body:\n\n```\n{ \"status\": \"ALREADY_COMPLETED\"}\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "58",
                      "type": "subsubsection",
                      "title": "Getting the response",
                      "content": "\nOnce you get the `COMPLETED` status, the `response` will be available along with its `logs`.\n\n```\ncurl -X GET https://queue.fal.run/fal-ai/fast-sdxl/requests/{request_id}\n```\n\nHere’s an example of a response with the `COMPLETED` status:\n\n```\n{ \"status\": \"COMPLETED\", \"logs\": [ { \"message\": \"2020-05-04 14:00:00.000000\", \"level\": \"INFO\", \"source\": \"stdout\", \"timestamp\": \"2020-05-04T14:00:00.000000Z\" } ], \"response\": { \"message\": \"Hello World!\" }}\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "59",
                      "type": "subsubsection",
                      "title": "Using webhook callbacks",
                      "content": "\nInstead of polling for the request status, you can have fal call a webhook when a request is finished. Please refer to the [Webhooks page](https://docs.fal.ai/model-endpoints/webhooks).",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Queue API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.15",
          "name": "Server-side integration API  fal.ai Reference  fal.ai Docs.md",
          "path": "01. fal/01.6 Model Endpoints/Server-side integration API  fal.ai Reference  fal.ai Docs.md",
          "sections": [
            {
              "id": "60",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Server-side integration API  fal.ai Reference  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "61",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Server-side integration API  fal.ai Reference  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "62",
                      "type": "subsubsection",
                      "title": "Ready-to-use proxy implementations",
                      "content": "\nWe provide ready-to-use proxy implementations for the following languages/frameworks:\n\n-   [Node.js with Next.js](https://docs.fal.ai/integrations/nextjs): a Next.js API route handler that can be used in any Next.js app. It supports both Page and App routers. We use it ourselves in all of our apps in production.\n-   [Node.js with Express](https://github.com/fal-ai/serverless-js/tree/main/apps/demo-express-app): an Express route handler that can be used in any Express app. You can also implement custom logic and compose together with your own handlers.\n\nThat’s it for now, but we are looking out for our community needs and will add more implementations in the future. If you have any requests, join our community in our [Discord server](https://discord.gg/fal-ai).\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Server-side integration API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "63",
                      "type": "subsubsection",
                      "title": "The proxy formula",
                      "content": "\nIn case fal doesn’t provide a plug-and-play proxy implementation for your language/framework, you can use the following formula to implement your own proxy:\n\n1.  Provide a single endpoint that will ingest all requests from the client (e.g. `/api/fal/proxy` is commonly used as the default route path).\n2.  The endpoint must support both `GET` and `POST` requests. When an unsupported HTTP method is used, the proxy must return status code `405`, Method Not Allowed.\n3.  The URL the proxy needs to call is provided by the `x-fal-target-url` header. If the header is missing, the proxy must return status code `400`, Bad Request. In case it doesn’t point to a valid URL, or the URL’s domain is not `*.fal.ai` or `*.fal.run`, the proxy must return status code `412`, Precondition Failed.\n4.  The request body, when present, is always in the JSON format - i.e. `content-type` header is `application/json`. Any other type of content must be rejected with status code `415`, Unsupported Media Type.\n5.  The proxy must add the `authorization` header in the format of `Key <your-api-key>` to the request it sends to the target URL. Your API key should be resolved from the environment variable `FAL_KEY`.\n6.  The response from the target URL will always be in the JSON format, the proxy must return the same response to the client.\n7.  The proxy must return the same HTTP status code as the target URL.\n8.  The proxy must return the same headers as the target URL, except for the `content-length` and `content-encoding` headers, which should be set by the your own server/framework automatically.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Server-side integration API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "64",
                      "type": "subsubsection",
                      "title": "Configure the client",
                      "content": "\nTo use the proxy, you need to configure the client to use the proxy endpoint. You can do that by setting the `proxyUrl` option in the client configuration:\n\n```\nimport { fal } from \"@fal-ai/client\";fal.config({ proxyUrl: \"/api/fal/proxy\",});\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Server-side integration API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "65",
                      "type": "subsubsection",
                      "title": "Example implementation",
                      "content": "\nYou can find a reference implementation of the proxy formula using TypeScript, which supports both Express and Next.js, in [serverless-js/libs/proxy/src/index.ts](https://github.com/fal-ai/serverless-js/blob/main/libs/proxy/src/index.ts).",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Server-side integration API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.16",
          "name": "Synchronous Requests API  fal.ai Reference  fal.ai Docs.md",
          "path": "01. fal/01.6 Model Endpoints/Synchronous Requests API  fal.ai Reference  fal.ai Docs.md",
          "sections": [
            {
              "id": "66",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Synchronous Requests API  fal.ai Reference  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "67",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Synchronous Requests API  fal.ai Reference  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "68",
                      "type": "subsubsection",
                      "title": "Submit a request",
                      "content": "\nHere is an example of using the curl command to submit a synchronous request:\n\n```\ncurl -X POST https://fal.run/fal-ai/fast-sdxl \\ -H \"Authorization: Key $FAL_KEY\" \\ -d '{\"prompt\": \"a cat\"}'\n```\n\nThe response will come directly from the model:\n\n```\n{ \"images\": [ { \"url\": \"https://v3.fal.media/files/rabbit/YYbm6L3DaXYHDL1_A4OaL.jpeg\", \"width\": 1024, \"height\": 1024, \"content_type\": \"image/jpeg\" } ], \"timings\": { \"inference\": 2.507048434985336 }, \"seed\": 15860307465884635512, \"has_nsfw_concepts\": [ false ], \"prompt\": \"a cat\"}\n```",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Synchronous Requests API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.17",
          "name": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
          "path": "01. fal/01.6 Model Endpoints/Webhooks API  fal.ai Reference  fal.ai Docs.md",
          "sections": [
            {
              "id": "69",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "70",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "71",
                      "type": "subsubsection",
                      "title": "Successful result",
                      "content": "\nThe following is an example of a successful request:\n\n```\n{ \"request_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"gateway_request_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"status\": \"OK\", \"payload\": { \"images\": [ { \"url\": \"https://url.to/image.png\", \"content_type\": \"image/png\", \"file_name\": \"image.png\", \"file_size\": 1824075, \"width\": 1024, \"height\": 1024 } ], \"seed\": 196619188014358660 }}\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "72",
                      "type": "subsubsection",
                      "title": "Response errors",
                      "content": "\nWhen an error happens, the `status` will be `ERROR`. The `error` property will contain a message and the `payload` will provide the error details. For example, if you forget to pass the required `model_name` parameter, you will get the following response:\n\n```\n{ \"request_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"gateway_request_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"status\": \"ERROR\", \"error\": \"Invalid status code: 422\", \"payload\": { \"detail\": [ { \"loc\": [\"body\", \"prompt\"], \"msg\": \"field required\", \"type\": \"value_error.missing\" } ] }}\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "73",
                      "type": "subsubsection",
                      "title": "Payload errors",
                      "content": "\nFor the webhook to include the payload, it must be valid JSON. So if there is an error serializing it, `payload` is set to `null` and a `payload_error` will include details about the error.\n\n```\n{ \"request_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"gateway_request_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"status\": \"OK\", \"payload\": null, \"payload_error\": \"Response payload is not JSON serializable. Either return a JSON serializable object or use the queue endpoint to retrieve the response.\"}\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "74",
                      "type": "subsubsection",
                      "title": "Retry policy",
                      "content": "\nIf the webhook fails to deliver the payload, it will retry 10 times in the span of 2 hours.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "75",
                      "type": "subsubsection",
                      "title": "Verifying Your Webhook",
                      "content": "\n-   **Caching the JWKS**: The JWKS can be cached for 24 hours to minimize network requests. The example implementations include basic in-memory caching.\n-   **Timestamp Validation**: The ±5-minute leeway ensures robustness against minor clock differences. Adjust this value if your use case requires stricter or looser validation.\n-   **Error Handling**: The examples include comprehensive error handling for missing headers, invalid signatures, and network issues. Log errors appropriately for debugging.\n-   **Framework Integration**: For frameworks like FastAPI (Python) or Express (JavaScript), ensure the raw request body is accessible. For Express, use `express.raw({ type: 'application/json' })` middleware before JSON parsing.",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Webhooks API  fal.ai Reference  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.18",
          "name": "Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
          "path": "01. fal/01.6 Model Endpoints/Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
          "sections": [
            {
              "id": "76",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "77",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "78",
                      "type": "subsubsection",
                      "title": "Workflow as an API",
                      "content": "\nWorkflow APIs work the same way as other model endpoints, you can simply send a request and get a response back. However, it is common for workflows to contain multiple steps and produce intermediate results, as each step contains their own response that could be relevant in your use-case.\n\nTherefore, workflows benefit from the **streaming** feature, which allows you to get partial results as they are being generated.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "79",
                      "type": "subsubsection",
                      "title": "Workflow events",
                      "content": "\nThe `error` event is triggered when an error occurs during the execution of a step. The `error` object contains the `error.status` with the HTTP status code, an error `message` as well as `error.body` with the underlying error serialized.\n\n```\n{ \"type\": \"error\", \"node_id\": \"stable_diffusion_xl\", \"message\": \"Error while fetching the result of the request d778bdf4-0275-47c2-9f23-16c27041cbeb\", \"error\": { \"status\": 422, \"body\": { \"detail\": [ { \"loc\": [\"body\", \"num_images\"], \"msg\": \"ensure this value is less than or equal to 8\", \"type\": \"value_error.number.not_le\", \"ctx\": { \"limit_value\": 8 } } ] } }}\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "80",
                      "type": "subsubsection",
                      "title": "Example",
                      "content": "\nA cool and simple example of the power of workflows is `workflows/fal-ai/sdxl-sticker`, which consists of three steps:\n\n1.  Generates an image using `fal-ai/fast-sdxl`.\n2.  Remove the background of the image using `fal-ai/imageutils/rembg`.\n3.  Converts the image to a sticker using `fal-ai/face-to-sticker`.\n\nWhat could be a tedious process of running and coordinating three different models is now a single endpoint that you can call with a single request.\n\n-   [Javascript](https://docs.fal.ai/model-endpoints/workflows#tab-panel-29)\n-   [python](https://docs.fal.ai/model-endpoints/workflows#tab-panel-30)\n-   [python (async)](https://docs.fal.ai/model-endpoints/workflows#tab-panel-31)\n-   [Swift](https://docs.fal.ai/model-endpoints/workflows#tab-panel-32)\n\n```\nimport fal_clientstream\n= fal_client.stream( \"workflows/fal-ai/sdxl-sticker\", arguments={ \"prompt\": \"a face of a cute puppy, in the style of pixar animation\", },)\nfor event in stream: print(event)\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "81",
                      "type": "subsubsection",
                      "title": "Type definitions",
                      "content": "\nBelow are the type definition in TypeScript of events that you can expect from a workflow stream:\n\n```\ntype WorkflowBaseEvent = { type: \"submit\" | \"completion\" | \"error\" | \"output\"; node_id: string;};export type WorkflowSubmitEvent = WorkflowBaseEvent &amp; { type: \"submit\"; app_id: string; request_id: string;};export type WorkflowCompletionEvent&lt;Output = any&gt; = WorkflowBaseEvent &amp; { type: \"completion\"; app_id: string; output: Output;};export type WorkflowDoneEvent&lt;Output = any&gt; = WorkflowBaseEvent &amp; { type: \"output\"; output: Output;};export type WorkflowErrorEvent = WorkflowBaseEvent &amp; { type: \"error\"; message: string; error: any;};\n```",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Workflow endpoints API  fal.ai Reference  fal.ai Docs (1).md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.19",
          "name": "Keeping fal API Secrets Safe  fal.ai Real-Time Models  fal.ai Docs.md",
          "path": "01. fal/01.7 Real Time Models/Keeping fal API Secrets Safe  fal.ai Real-Time Models  fal.ai Docs.md",
          "sections": []
        },
        {
          "id": "01.20",
          "name": "Real Time Models Quickstart  fal.ai Real-Time Models  fal.ai Docs.md",
          "path": "01. fal/01.7 Real Time Models/Real Time Models Quickstart  fal.ai Real-Time Models  fal.ai Docs.md",
          "sections": []
        },
        {
          "id": "01.21",
          "name": "Real-Time Models  fal.ai Real-Time Models  fal.ai Docs.md",
          "path": "01. fal/01.7 Real Time Models/Real-Time Models  fal.ai Real-Time Models  fal.ai Docs.md",
          "sections": []
        },
        {
          "id": "01.22",
          "name": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
          "path": "01. fal/01.8 Integration/Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
          "sections": [
            {
              "id": "82",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "83",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "84",
                      "type": "subsubsection",
                      "title": "You will learn how to:",
                      "content": "\n1.  Install the fal.ai libraries\n2.  Add a server proxy to protect your credentials\n3.  Generate an image using SDXL\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "85",
                      "type": "subsubsection",
                      "title": "Prerequisites",
                      "content": "\n1.  Have an existing Next.js app or create a new one using `npx create-next-app`\n2.  Have a [fal.ai](https://fal.ai/) account\n3.  Have an API Key. You can [create one here](https://fal.ai/dashboard/keys)\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "86",
                      "type": "subsubsection",
                      "title": "1\\. Install the fal.ai libraries",
                      "content": "\nUsing your favorite package manager, install both the `@fal-ai/client` and `@fal-ai/server-proxy` libraries.\n\n-   [npm](https://docs.fal.ai/integrations/nextjs#tab-panel-24)\n-   [yarn](https://docs.fal.ai/integrations/nextjs#tab-panel-25)\n-   [pnpm](https://docs.fal.ai/integrations/nextjs#tab-panel-26)\n\n```\nnpm install @fal-ai/client @fal-ai/server-proxy\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "87",
                      "type": "subsubsection",
                      "title": "2\\. Setup the proxy",
                      "content": "\nIt’s common for applications to execute custom logic before or after the proxy handler. For example, you may want to add a custom header to the request, or log the request and response, or apply some rate limit. The good news is that the proxy implementation is simply a standard Next.js API/route handler function, which means you can compose it with other handlers.\n\nFor example, let’s assume you want to add some analytics and apply some rate limit to the proxy handler:\n\n```\nimport { route } from \"@fal-ai/server-proxy/nextjs\";// Let's add some custom logic to POST requests - i.e. when the request is// submitted for processingexport const POST = (req) \n=&gt; { // Add some analytics analytics.track(\"fal.ai request\", { targetUrl: req.headers[\"x-fal-target-url\"], userId: req.user.id, })\n; // Apply some rate limit if (rateLimiter.shouldLimit(req)\n) \n{ res.status(429)\n.json({ error: \"Too many requests\" })\n; } // If everything passed your custom logic,\nnow execute the proxy handler return route.POST(req)\n;};// For GET requests we will just use the built-in proxy handler// But you could also add some custom logic here if you needexport const GET = route.GET;\n```\n\nNote that the URL that will be forwarded to server is available as a header named `x-fal-target-url`. Also, keep in mind the example above is just an example, `rateLimiter` and `analytics` are just placeholders.\n\nThe example above used the app router, but the same logic can be applied to the page router and its `handler` function.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "88",
                      "type": "subsubsection",
                      "title": "3\\. Configure the client",
                      "content": "\nOn your main file (i.e. `src/pages/_app.jsx` or `src/app/page.jsx`), configure the client to use the proxy:\n\n```\nimport { fal } from \"@fal-ai/client\";fal.config({ proxyUrl: \"/api/fal/proxy\",});\n```\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "89",
                      "type": "subsubsection",
                      "title": "4\\. Generate an image",
                      "content": "\nNow that the client is configured, you can generate an image using `fal.subscribe` and pass the model id and the input parameters:\n\n```\nconst result = await fal.subscribe(\"fal-ai/flux/dev\", { input: { prompt, image_size: \"square_hd\", }, pollInterval: 5000, logs: true, onQueueUpdate(update) \n{ console.log(\"queue update\", update)\n; },\n})\n;const imageUrl = result.images[0].url;\n```\n\nSee more about Flux Dev used in this example on [fal.ai/models/fal-ai/flux/dev](https://fal.ai/models/fal-ai/flux/dev).\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "90",
                      "type": "subsubsection",
                      "title": "What’s next?",
                      "content": "\nImage generation is just one of the many cool things you can do with fal. Make sure you:\n\n-   Check our demo application at [github.com/fal-ai/serverless-js/apps/demo-nextjs-app-router](https://github.com/fal-ai/fal-js/tree/main/apps/demo-nextjs-app-router)\n-   Check all the available [Model APIs](https://fal.ai/models)\n-   Learn how to write your own model APIs on [Introduction to serverless functions](https://docs.fal.ai/private-serverless-apps)\n-   Read more about function endpoints on [private serverless models](https://docs.fal.ai/private-serverless-apps)\n-   Check the next page to learn how to [deploy your app to Vercel](https://docs.fal.ai/integrations/vercel)",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.23",
          "name": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
          "path": "01. fal/01.8 Integration/Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
          "sections": [
            {
              "id": "91",
              "type": "section",
              "title": "Main Content",
              "content": "",
              "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "92",
                  "type": "subsection",
                  "title": "Subsection",
                  "content": "",
                  "parent_section": "Main Content",
                  "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "93",
                      "type": "subsubsection",
                      "title": "You will learn how to:",
                      "content": "\n-   Connect a Next.js app deployed on Vercel to fal.ai\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "94",
                      "type": "subsubsection",
                      "title": "Prerequisites",
                      "content": "\n1.  A [fal.ai](https://fal.ai/) account\n2.  A [Vercel account](https://vercel.com/)\n3.  A Next.js app. Check the [Next.js guide](https://docs.fal.ai/integrations/nextjs) if you don’t have one yet.\n4.  App deployed on Vercel. Run `npx vercel` in your app directory to deploy it in case you haven’t done it yet.\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "95",
                      "type": "subsubsection",
                      "title": "Vercel official integration",
                      "content": "\nThe recommended way to add fal.ai to your app deployed on Vercel is to use the official integration. You can find it in the [Vercel marketplace](https://vercel.com/integrations/fal).\n\nClick on **Add integration** and follow the steps. After you’re done, re-deploy your app and you’re good to go!\n\n![Vercel integration](https://integrations-og-image.vercel.sh/api/og/fal?42673700034a7509d66487f3ed68a2bd)\n",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "96",
                      "type": "subsubsection",
                      "title": "Manual setup",
                      "content": "\nYou can also manually add fal credentials to your Vercel environment manually.\n\n1.  Go to your [fal.ai dashboard](https://fal.ai/dashboard/keys), create an **API-scoped** key and copy it. Make sure you create an alias do identify which app is using it.\n2.  Go to your app settings in Vercel and add a new environment variable called `FAL_KEY` with the value of the key you just copied. You can choose other names, but keep in mind that the default convention of fal-provided libraries is `FAL_KEY`.\n3.  Re-deploy your app and you’re good to go!",
                      "parent_subsection": "Subsection",
                      "parent_section": "Main Content",
                      "file": "Add fal.ai to your Next.js app Integration  fal.ai Docs  fal.ai Docs2.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.24",
          "name": "flux-pro_v1.1-ultra.md",
          "path": "02. Models/flux-pro_v1.1-ultra.md",
          "sections": [
            {
              "id": "97",
              "type": "section",
              "title": "FLUX1.1 [pro] ultra",
              "content": "\n> FLUX1.1 [pro] ultra is the newest version of FLUX1.1 [pro], maintaining professional-grade image quality while delivering up to 2K resolution with improved photo realism.\n\n",
              "file": "flux-pro_v1.1-ultra.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "98",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/flux-pro/v1.1-ultra`\n- **Model ID**: `fal-ai/flux-pro/v1.1-ultra`\n- **Category**: text-to-image\n- **Kind**: inference\n**Tags**: high-res, realism\n\n\n",
                  "parent_section": "FLUX1.1 [pro] ultra",
                  "file": "flux-pro_v1.1-ultra.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "99",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "FLUX1.1 [pro] ultra",
                  "file": "flux-pro_v1.1-ultra.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "100",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The prompt to generate an image from.\n  - Examples: \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n\n- **`seed`** (`integer`, _optional_):\n  The same seed and the same prompt given to the same version of the model\n  will output the same image every time.\n\n- **`sync_mode`** (`boolean`, _optional_):\n  If set to true, the function will wait for the image to be generated and uploaded\n  before returning the response. This will increase the latency of the function but\n  it allows you to get the image directly in the response without going through the CDN.\n  - Default: `false`\n\n- **`num_images`** (`integer`, _optional_):\n  The number of images to generate. Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`enable_safety_checker`** (`boolean`, _optional_):\n  If set to true, the safety checker will be enabled. Default value: `true`\n  - Default: `true`\n\n- **`output_format`** (`OutputFormatEnum`, _optional_):\n  The format of the generated image. Default value: `\"jpeg\"`\n  - Default: `\"jpeg\"`\n  - Options: `\"jpeg\"`, `\"png\"`\n\n- **`safety_tolerance`** (`SafetyToleranceEnum`, _optional_):\n  The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n  - Default: `\"2\"`\n  - Options: `\"1\"`, `\"2\"`, `\"3\"`, `\"4\"`, `\"5\"`, `\"6\"`\n\n- **`aspect_ratio`** (`Enum | string`, _optional_):\n  The aspect ratio of the generated image. Default value: `16:9`\n  - Default: `\"16:9\"`\n  - One of: Enum | string\n\n- **`raw`** (`boolean`, _optional_):\n  Generate less processed, more natural-looking images.\n  - Default: `false`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\",\n  \"num_images\": 1,\n  \"enable_safety_checker\": true,\n  \"output_format\": \"jpeg\",\n  \"safety_tolerance\": \"2\",\n  \"aspect_ratio\": \"16:9\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "FLUX1.1 [pro] ultra",
                      "file": "flux-pro_v1.1-ultra.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "101",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "FLUX1.1 [pro] ultra",
                  "file": "flux-pro_v1.1-ultra.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "102",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/flux-pro/v1.1-ultra \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX1.1 [pro] ultra",
                      "file": "flux-pro_v1.1-ultra.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "103",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/flux-pro/v1.1-ultra\",\n    arguments={\n        \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX1.1 [pro] ultra",
                      "file": "flux-pro_v1.1-ultra.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "104",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "FLUX1.1 [pro] ultra",
                  "file": "flux-pro_v1.1-ultra.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "105",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/flux-pro/v1.1-ultra)\n- [API Documentation](https://fal.ai/models/fal-ai/flux-pro/v1.1-ultra/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/flux-pro/v1.1-ultra)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX1.1 [pro] ultra",
                      "file": "flux-pro_v1.1-ultra.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "106",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX1.1 [pro] ultra",
                      "file": "flux-pro_v1.1-ultra.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.25",
          "name": "flux1_kontext_dev.md",
          "path": "02. Models/flux1_kontext_dev.md",
          "sections": [
            {
              "id": "107",
              "type": "section",
              "title": "FLUX.1 Kontext [dev]",
              "content": "\n> Frontier image editing model.\n\n",
              "file": "flux1_kontext_dev.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "108",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/flux-kontext/dev`\n- **Model ID**: `fal-ai/flux-kontext/dev`\n- **Category**: image-to-image\n- **Kind**: inference\n\n",
                  "parent_section": "FLUX.1 Kontext [dev]",
                  "file": "flux1_kontext_dev.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "109",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "FLUX.1 Kontext [dev]",
                  "file": "flux1_kontext_dev.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "110",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The prompt to edit the image.\n  - Examples: \"Change the setting to a day time, add a lot of people walking the sidewalk while maintaining the same style of the painting\"\n\n- **`image_url`** (`string`, _required_):\n  The URL of the image to edit.\n  - Examples: \"https://storage.googleapis.com/falserverless/example_inputs/kontext_example_input.webp\"\n\n- **`num_inference_steps`** (`integer`, _optional_):\n  The number of inference steps to perform. Default value: `28`\n  - Default: `28`\n  - Range: `10` to `50`\n\n- **`seed`** (`integer`, _optional_):\n  The same seed and the same prompt given to the same version of the model\n  will output the same image every time.\n\n- **`guidance_scale`** (`float`, _optional_):\n  The CFG (Classifier Free Guidance) scale is a measure of how close you want\n  the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n  - Default: `2.5`\n  - Range: `1` to `20`\n\n- **`sync_mode`** (`boolean`, _optional_):\n  If set to true, the function will wait for the image to be generated and uploaded\n  before returning the response. This will increase the latency of the function but\n  it allows you to get the image directly in the response without going through the CDN.\n  - Default: `false`\n\n- **`num_images`** (`integer`, _optional_):\n  The number of images to generate. Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`enable_safety_checker`** (`boolean`, _optional_):\n  If set to true, the safety checker will be enabled. Default value: `true`\n  - Default: `true`\n\n- **`output_format`** (`OutputFormatEnum`, _optional_):\n  Output format Default value: `\"jpeg\"`\n  - Default: `\"jpeg\"`\n  - Options: `\"jpeg\"`, `\"png\"`\n\n- **`acceleration`** (`AccelerationEnum`, _optional_):\n  The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n  - Default: `\"none\"`\n  - Options: `\"none\"`, `\"regular\"`, `\"high\"`\n\n- **`resolution_mode`** (`ResolutionModeEnum`, _optional_):\n  Determines how the output resolution is set for image editing.\n  - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.\n  - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).\n  Apart from these, a few aspect ratios are also supported. Default value: `\"match_input\"`\n  - Default: `\"match_input\"`\n  - Options: `\"auto\"`, `\"match_input\"`, `\"1:1\"`, `\"16:9\"`, `\"21:9\"`, `\"3:2\"`, `\"2:3\"`, `\"4:5\"`, `\"5:4\"`, `\"3:4\"`, `\"4:3\"`, `\"9:16\"`, `\"9:21\"`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Change the setting to a day time, add a lot of people walking the sidewalk while maintaining the same style of the painting\",\n  \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/kontext_example_input.webp\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Change the setting to a day time, add a lot of people walking the sidewalk while maintaining the same style of the painting\",\n  \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/kontext_example_input.webp\",\n  \"num_inference_steps\": 28,\n  \"guidance_scale\": 2.5,\n  \"num_images\": 1,\n  \"enable_safety_checker\": true,\n  \"output_format\": \"jpeg\",\n  \"acceleration\": \"none\",\n  \"resolution_mode\": \"match_input\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "FLUX.1 Kontext [dev]",
                      "file": "flux1_kontext_dev.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "111",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [dev]",
                  "file": "flux1_kontext_dev.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "112",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/flux-kontext/dev \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Change the setting to a day time, add a lot of people walking the sidewalk while maintaining the same style of the painting\",\n     \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/kontext_example_input.webp\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [dev]",
                      "file": "flux1_kontext_dev.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "113",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/flux-kontext/dev\",\n    arguments={\n        \"prompt\": \"Change the setting to a day time, add a lot of people walking the sidewalk while maintaining the same style of the painting\",\n        \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/kontext_example_input.webp\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [dev]",
                      "file": "flux1_kontext_dev.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "114",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [dev]",
                  "file": "flux1_kontext_dev.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "115",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/flux-kontext/dev)\n- [API Documentation](https://fal.ai/models/fal-ai/flux-kontext/dev/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/flux-kontext/dev)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [dev]",
                      "file": "flux1_kontext_dev.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "116",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [dev]",
                      "file": "flux1_kontext_dev.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.26",
          "name": "flux1_kontext_max_editing.md",
          "path": "02. Models/flux1_kontext_max_editing.md",
          "sections": [
            {
              "id": "117",
              "type": "section",
              "title": "FLUX.1 Kontext [max]",
              "content": "\n> FLUX.1 Kontext [max] is a model with greatly improved prompt adherence and typography generation meet premium consistency for editing without compromise on speed. \n \n\n",
              "file": "flux1_kontext_max_editing.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "118",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/flux-pro/kontext/max`\n- **Model ID**: `fal-ai/flux-pro/kontext/max`\n- **Category**: image-to-image\n- **Kind**: inference\n\n",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_editing.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "119",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_editing.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "120",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The prompt to generate an image from.\n  - Examples: \"Put a donut next to the flour.\"\n\n- **`seed`** (`integer`, _optional_):\n  The same seed and the same prompt given to the same version of the model\n  will output the same image every time.\n\n- **`guidance_scale`** (`float`, _optional_):\n  The CFG (Classifier Free Guidance) scale is a measure of how close you want\n  the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n  - Default: `3.5`\n  - Range: `1` to `20`\n\n- **`sync_mode`** (`boolean`, _optional_):\n  If set to true, the function will wait for the image to be generated and uploaded\n  before returning the response. This will increase the latency of the function but\n  it allows you to get the image directly in the response without going through the CDN.\n  - Default: `false`\n\n- **`num_images`** (`integer`, _optional_):\n  The number of images to generate. Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`output_format`** (`OutputFormatEnum`, _optional_):\n  The format of the generated image. Default value: `\"jpeg\"`\n  - Default: `\"jpeg\"`\n  - Options: `\"jpeg\"`, `\"png\"`\n\n- **`safety_tolerance`** (`SafetyToleranceEnum`, _optional_):\n  The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n  - Default: `\"2\"`\n  - Options: `\"1\"`, `\"2\"`, `\"3\"`, `\"4\"`, `\"5\"`, `\"6\"`\n\n- **`aspect_ratio`** (`AspectRatioEnum`, _optional_):\n  The aspect ratio of the generated image.\n  - Options: `\"21:9\"`, `\"16:9\"`, `\"4:3\"`, `\"3:2\"`, `\"1:1\"`, `\"2:3\"`, `\"3:4\"`, `\"9:16\"`, `\"9:21\"`\n\n- **`image_url`** (`string`, _required_):\n  Image prompt for the omni model.\n  - Examples: \"https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png\"\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Put a donut next to the flour.\",\n  \"image_url\": \"https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Put a donut next to the flour.\",\n  \"guidance_scale\": 3.5,\n  \"num_images\": 1,\n  \"output_format\": \"jpeg\",\n  \"safety_tolerance\": \"2\",\n  \"image_url\": \"https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_editing.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "121",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_editing.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "122",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/flux-pro/kontext/max \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Put a donut next to the flour.\",\n     \"image_url\": \"https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_editing.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "123",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/flux-pro/kontext/max\",\n    arguments={\n        \"prompt\": \"Put a donut next to the flour.\",\n        \"image_url\": \"https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_editing.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "124",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_editing.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "125",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/flux-pro/kontext/max)\n- [API Documentation](https://fal.ai/models/fal-ai/flux-pro/kontext/max/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/flux-pro/kontext/max)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_editing.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "126",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_editing.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.27",
          "name": "flux1_kontext_max_multi-img.md",
          "path": "02. Models/flux1_kontext_max_multi-img.md",
          "sections": [
            {
              "id": "127",
              "type": "section",
              "title": "FLUX.1 Kontext [max]",
              "content": "\n> Experimental version of FLUX.1 Kontext [max] with multi image handling capabilities\n\n",
              "file": "flux1_kontext_max_multi-img.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "128",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/flux-pro/kontext/max/multi`\n- **Model ID**: `fal-ai/flux-pro/kontext/max/multi`\n- **Category**: image-to-image\n- **Kind**: inference\n\n",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_multi-img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "129",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_multi-img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "130",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The prompt to generate an image from.\n  - Examples: \"Put the little duckling on top of the woman's t-shirt.\"\n\n- **`seed`** (`integer`, _optional_):\n  The same seed and the same prompt given to the same version of the model\n  will output the same image every time.\n\n- **`guidance_scale`** (`float`, _optional_):\n  The CFG (Classifier Free Guidance) scale is a measure of how close you want\n  the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n  - Default: `3.5`\n  - Range: `1` to `20`\n\n- **`sync_mode`** (`boolean`, _optional_):\n  If set to true, the function will wait for the image to be generated and uploaded\n  before returning the response. This will increase the latency of the function but\n  it allows you to get the image directly in the response without going through the CDN.\n  - Default: `false`\n\n- **`num_images`** (`integer`, _optional_):\n  The number of images to generate. Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`output_format`** (`OutputFormatEnum`, _optional_):\n  The format of the generated image. Default value: `\"jpeg\"`\n  - Default: `\"jpeg\"`\n  - Options: `\"jpeg\"`, `\"png\"`\n\n- **`safety_tolerance`** (`SafetyToleranceEnum`, _optional_):\n  The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n  - Default: `\"2\"`\n  - Options: `\"1\"`, `\"2\"`, `\"3\"`, `\"4\"`, `\"5\"`, `\"6\"`\n\n- **`aspect_ratio`** (`AspectRatioEnum`, _optional_):\n  The aspect ratio of the generated image.\n  - Options: `\"21:9\"`, `\"16:9\"`, `\"4:3\"`, `\"3:2\"`, `\"1:1\"`, `\"2:3\"`, `\"3:4\"`, `\"9:16\"`, `\"9:21\"`\n\n- **`image_urls`** (`list<string>`, _required_):\n  Image prompt for the omni model.\n  - Array of string\n  - Examples: [\"https://v3.fal.media/files/penguin/XoW0qavfF-ahg-jX4BMyL_image.webp\",\"https://v3.fal.media/files/tiger/bml6YA7DWJXOigadvxk75_image.webp\"]\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Put the little duckling on top of the woman's t-shirt.\",\n  \"image_urls\": [\n    \"https://v3.fal.media/files/penguin/XoW0qavfF-ahg-jX4BMyL_image.webp\",\n    \"https://v3.fal.media/files/tiger/bml6YA7DWJXOigadvxk75_image.webp\"\n  ]\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Put the little duckling on top of the woman's t-shirt.\",\n  \"guidance_scale\": 3.5,\n  \"num_images\": 1,\n  \"output_format\": \"jpeg\",\n  \"safety_tolerance\": \"2\",\n  \"image_urls\": [\n    \"https://v3.fal.media/files/penguin/XoW0qavfF-ahg-jX4BMyL_image.webp\",\n    \"https://v3.fal.media/files/tiger/bml6YA7DWJXOigadvxk75_image.webp\"\n  ]\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_multi-img.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "131",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_multi-img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "132",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/flux-pro/kontext/max/multi \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Put the little duckling on top of the woman's t-shirt.\",\n     \"image_urls\": [\n       \"https://v3.fal.media/files/penguin/XoW0qavfF-ahg-jX4BMyL_image.webp\",\n       \"https://v3.fal.media/files/tiger/bml6YA7DWJXOigadvxk75_image.webp\"\n     ]\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_multi-img.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "133",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/flux-pro/kontext/max/multi\",\n    arguments={\n        \"prompt\": \"Put the little duckling on top of the woman's t-shirt.\",\n        \"image_urls\": [\"https://v3.fal.media/files/penguin/XoW0qavfF-ahg-jX4BMyL_image.webp\", \"https://v3.fal.media/files/tiger/bml6YA7DWJXOigadvxk75_image.webp\"]\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_multi-img.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "134",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_multi-img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "135",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/flux-pro/kontext/max/multi)\n- [API Documentation](https://fal.ai/models/fal-ai/flux-pro/kontext/max/multi/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/flux-pro/kontext/max/multi)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_multi-img.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "136",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_multi-img.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.28",
          "name": "flux1_kontext_max_txt-_img.md",
          "path": "02. Models/flux1_kontext_max_txt-_img.md",
          "sections": [
            {
              "id": "137",
              "type": "section",
              "title": "FLUX.1 Kontext [max]",
              "content": "\n> FLUX.1 Kontext [max] text-to-image is a new premium model brings maximum performance across all aspects – greatly improved prompt adherence.\n\n",
              "file": "flux1_kontext_max_txt-_img.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "138",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/flux-pro/kontext/max/text-to-image`\n- **Model ID**: `fal-ai/flux-pro/kontext/max/text-to-image`\n- **Category**: text-to-image\n- **Kind**: inference\n\n",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_txt-_img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "139",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_txt-_img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "140",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The prompt to generate an image from.\n  - Examples: \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n\n- **`seed`** (`integer`, _optional_):\n  The same seed and the same prompt given to the same version of the model\n  will output the same image every time.\n\n- **`guidance_scale`** (`float`, _optional_):\n  The CFG (Classifier Free Guidance) scale is a measure of how close you want\n  the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n  - Default: `3.5`\n  - Range: `1` to `20`\n\n- **`sync_mode`** (`boolean`, _optional_):\n  If set to true, the function will wait for the image to be generated and uploaded\n  before returning the response. This will increase the latency of the function but\n  it allows you to get the image directly in the response without going through the CDN.\n  - Default: `false`\n\n- **`num_images`** (`integer`, _optional_):\n  The number of images to generate. Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`output_format`** (`OutputFormatEnum`, _optional_):\n  The format of the generated image. Default value: `\"jpeg\"`\n  - Default: `\"jpeg\"`\n  - Options: `\"jpeg\"`, `\"png\"`\n\n- **`safety_tolerance`** (`SafetyToleranceEnum`, _optional_):\n  The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n  - Default: `\"2\"`\n  - Options: `\"1\"`, `\"2\"`, `\"3\"`, `\"4\"`, `\"5\"`, `\"6\"`\n\n- **`aspect_ratio`** (`AspectRatioEnum`, _optional_):\n  The aspect ratio of the generated image. Default value: `\"1:1\"`\n  - Default: `\"1:1\"`\n  - Options: `\"21:9\"`, `\"16:9\"`, `\"4:3\"`, `\"3:2\"`, `\"1:1\"`, `\"2:3\"`, `\"3:4\"`, `\"9:16\"`, `\"9:21\"`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\",\n  \"guidance_scale\": 3.5,\n  \"num_images\": 1,\n  \"output_format\": \"jpeg\",\n  \"safety_tolerance\": \"2\",\n  \"aspect_ratio\": \"1:1\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_txt-_img.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "141",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_txt-_img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "142",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/flux-pro/kontext/max/text-to-image \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_txt-_img.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "143",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/flux-pro/kontext/max/text-to-image\",\n    arguments={\n        \"prompt\": \"Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word \\\"FLUX\\\" is painted over it in big, white brush strokes with visible texture.\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_txt-_img.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "144",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "FLUX.1 Kontext [max]",
                  "file": "flux1_kontext_max_txt-_img.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "145",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/flux-pro/kontext/max/text-to-image)\n- [API Documentation](https://fal.ai/models/fal-ai/flux-pro/kontext/max/text-to-image/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/flux-pro/kontext/max/text-to-image)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_txt-_img.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "146",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "FLUX.1 Kontext [max]",
                      "file": "flux1_kontext_max_txt-_img.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.29",
          "name": "flux1_kontext_trainer.md",
          "path": "02. Models/flux1_kontext_trainer.md",
          "sections": [
            {
              "id": "147",
              "type": "section",
              "title": "Flux Kontext Trainer",
              "content": "\n> LoRA trainer for FLUX.1 Kontext [dev]\n\n",
              "file": "flux1_kontext_trainer.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "148",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/flux-kontext-trainer`\n- **Model ID**: `fal-ai/flux-kontext-trainer`\n- **Category**: training\n- **Kind**: training\n**Description**: LoRA trainer for FLUX.1 Kontext [dev]. Train custom LoRAs to extend the image editing functionality of FLUX.1 Kontext [dev]\n\n\n",
                  "parent_section": "Flux Kontext Trainer",
                  "file": "flux1_kontext_trainer.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "149",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Flux Kontext Trainer",
                  "file": "flux1_kontext_trainer.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "150",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`image_data_url`** (`string`, _required_):\n  URL to the input data zip archive.\n  \n  The zip should contain pairs of images. The images should be named:\n  \n  ROOT_start.EXT and ROOT_end.EXT\n  For example:\n  photo_start.jpg and photo_end.jpg\n  \n  The zip can also contain a text file for each image pair. The text file should be named:\n  ROOT.txt\n  For example:\n  photo.txt\n  \n  This text file can be used to specify the edit instructions for the image pair.\n  \n  If no text file is provided, the default_caption will be used.\n  \n  If no default_caption is provided, the training will fail.\n\n- **`steps`** (`integer`, _optional_):\n  Number of steps to train for Default value: `1000`\n  - Default: `1000`\n  - Range: `2` to `20000`\n\n- **`learning_rate`** (`float`, _optional_):\n   Default value: `0.0001`\n  - Default: `0.0001`\n\n- **`default_caption`** (`string`, _optional_):\n  Default caption to use when caption files are missing. If None, missing captions will cause an error.\n\n- **`output_lora_format`** (`OutputLoraFormatEnum`, _optional_):\n  Dictates the naming scheme for the output weights Default value: `\"fal\"`\n  - Default: `\"fal\"`\n  - Options: `\"fal\"`, `\"comfy\"`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"image_data_url\": \"\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"image_data_url\": \"\",\n  \"steps\": 1000,\n  \"learning_rate\": 0.0001,\n  \"output_lora_format\": \"fal\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Flux Kontext Trainer",
                      "file": "flux1_kontext_trainer.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "151",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Flux Kontext Trainer",
                  "file": "flux1_kontext_trainer.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "152",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/flux-kontext-trainer \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"image_data_url\": \"\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Flux Kontext Trainer",
                      "file": "flux1_kontext_trainer.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "153",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/flux-kontext-trainer\",\n    arguments={\n        \"image_data_url\": \"\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Flux Kontext Trainer",
                      "file": "flux1_kontext_trainer.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "154",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Flux Kontext Trainer",
                  "file": "flux1_kontext_trainer.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "155",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/flux-kontext-trainer)\n- [API Documentation](https://fal.ai/models/fal-ai/flux-kontext-trainer/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/flux-kontext-trainer)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Flux Kontext Trainer",
                      "file": "flux1_kontext_trainer.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "156",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Flux Kontext Trainer",
                      "file": "flux1_kontext_trainer.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.30",
          "name": "gpt-Image-1.md",
          "path": "02. Models/gpt-Image-1.md",
          "sections": [
            {
              "id": "157",
              "type": "section",
              "title": "gpt-image-1",
              "content": "\n> OpenAI's latest image generation and editing model: gpt-1-image. Currently powered with bring-your-own-key.\n\n",
              "file": "gpt-Image-1.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "158",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/gpt-image-1/text-to-image/byok`\n- **Model ID**: `fal-ai/gpt-image-1/text-to-image/byok`\n- **Category**: text-to-image\n- **Kind**: inference\n\n",
                  "parent_section": "gpt-image-1",
                  "file": "gpt-Image-1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "159",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "gpt-image-1",
                  "file": "gpt-Image-1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "160",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The prompt to generate the image from.\n  - Examples: \"A serene cyberpunk cityscape at twilight, with neon signs glowing in vibrant blues and purples, reflecting on rain-slick streets. Sleek futuristic buildings tower above, connected by glowing skybridges. A lone figure in a hooded jacket stands under a streetlamp, backlit by soft mist. The atmosphere is cinematic, moody\"\n\n- **`image_size`** (`ImageSizeEnum`, _optional_):\n  The size of the image to generate. Default value: `\"auto\"`\n  - Default: `\"auto\"`\n  - Options: `\"auto\"`, `\"1024x1024\"`, `\"1536x1024\"`, `\"1024x1536\"`\n\n- **`num_images`** (`integer`, _optional_):\n  The number of images to generate. Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`quality`** (`QualityEnum`, _optional_):\n  The quality of the image to generate. Default value: `\"auto\"`\n  - Default: `\"auto\"`\n  - Options: `\"auto\"`, `\"low\"`, `\"medium\"`, `\"high\"`\n\n- **`background`** (`BackgroundEnum`, _optional_):\n  The background of the image to generate. Default value: `\"auto\"`\n  - Default: `\"auto\"`\n  - Options: `\"auto\"`, `\"transparent\"`, `\"opaque\"`\n\n- **`openai_api_key`** (`string`, _required_):\n  The OpenAI API key to use for the image generation. This endpoint is currently powered by bring-your-own-key system.\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"A serene cyberpunk cityscape at twilight, with neon signs glowing in vibrant blues and purples, reflecting on rain-slick streets. Sleek futuristic buildings tower above, connected by glowing skybridges. A lone figure in a hooded jacket stands under a streetlamp, backlit by soft mist. The atmosphere is cinematic, moody\",\n  \"openai_api_key\": \"\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"A serene cyberpunk cityscape at twilight, with neon signs glowing in vibrant blues and purples, reflecting on rain-slick streets. Sleek futuristic buildings tower above, connected by glowing skybridges. A lone figure in a hooded jacket stands under a streetlamp, backlit by soft mist. The atmosphere is cinematic, moody\",\n  \"image_size\": \"auto\",\n  \"num_images\": 1,\n  \"quality\": \"auto\",\n  \"background\": \"auto\",\n  \"openai_api_key\": \"\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "gpt-image-1",
                      "file": "gpt-Image-1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "161",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "gpt-image-1",
                  "file": "gpt-Image-1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "162",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/gpt-image-1/text-to-image/byok \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"A serene cyberpunk cityscape at twilight, with neon signs glowing in vibrant blues and purples, reflecting on rain-slick streets. Sleek futuristic buildings tower above, connected by glowing skybridges. A lone figure in a hooded jacket stands under a streetlamp, backlit by soft mist. The atmosphere is cinematic, moody\",\n     \"openai_api_key\": \"\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "gpt-image-1",
                      "file": "gpt-Image-1.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "163",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/gpt-image-1/text-to-image/byok\",\n    arguments={\n        \"prompt\": \"A serene cyberpunk cityscape at twilight, with neon signs glowing in vibrant blues and purples, reflecting on rain-slick streets. Sleek futuristic buildings tower above, connected by glowing skybridges. A lone figure in a hooded jacket stands under a streetlamp, backlit by soft mist. The atmosphere is cinematic, moody\",\n        \"openai_api_key\": \"\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "gpt-image-1",
                      "file": "gpt-Image-1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "164",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "gpt-image-1",
                  "file": "gpt-Image-1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "165",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/gpt-image-1/text-to-image/byok)\n- [API Documentation](https://fal.ai/models/fal-ai/gpt-image-1/text-to-image/byok/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/gpt-image-1/text-to-image/byok)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "gpt-image-1",
                      "file": "gpt-Image-1.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "166",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "gpt-image-1",
                      "file": "gpt-Image-1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.31",
          "name": "hailou2.md",
          "path": "02. Models/hailou2.md",
          "sections": [
            {
              "id": "167",
              "type": "section",
              "title": "MiniMax Hailuo 02 [Pro] (Image to Video)",
              "content": "\n> MiniMax Hailuo-02 Image To Video API (Pro, 1080p): Advanced image-to-video generation model with 1080p resolution\n\n",
              "file": "hailou2.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "168",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/minimax/hailuo-02/pro/image-to-video`\n- **Model ID**: `fal-ai/minimax/hailuo-02/pro/image-to-video`\n- **Category**: image-to-video\n- **Kind**: inference\n\n",
                  "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                  "file": "hailou2.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "169",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                  "file": "hailou2.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "170",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_)\n  - Examples: \"Man walked into winter cave with polar bear\"\n\n- **`image_url`** (`string`, _required_)\n  - Examples: \"https://storage.googleapis.com/falserverless/model_tests/minimax/1749891352437225630-389852416840474630_1749891352.png\"\n\n- **`prompt_optimizer`** (`boolean`, _optional_):\n  Whether to use the model's prompt optimizer Default value: `true`\n  - Default: `true`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Man walked into winter cave with polar bear\",\n  \"image_url\": \"https://storage.googleapis.com/falserverless/model_tests/minimax/1749891352437225630-389852416840474630_1749891352.png\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Man walked into winter cave with polar bear\",\n  \"image_url\": \"https://storage.googleapis.com/falserverless/model_tests/minimax/1749891352437225630-389852416840474630_1749891352.png\",\n  \"prompt_optimizer\": true\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                      "file": "hailou2.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "171",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                  "file": "hailou2.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "172",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/minimax/hailuo-02/pro/image-to-video \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Man walked into winter cave with polar bear\",\n     \"image_url\": \"https://storage.googleapis.com/falserverless/model_tests/minimax/1749891352437225630-389852416840474630_1749891352.png\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                      "file": "hailou2.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "173",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/minimax/hailuo-02/pro/image-to-video\",\n    arguments={\n        \"prompt\": \"Man walked into winter cave with polar bear\",\n        \"image_url\": \"https://storage.googleapis.com/falserverless/model_tests/minimax/1749891352437225630-389852416840474630_1749891352.png\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                      "file": "hailou2.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "174",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                  "file": "hailou2.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "175",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/minimax/hailuo-02/pro/image-to-video)\n- [API Documentation](https://fal.ai/models/fal-ai/minimax/hailuo-02/pro/image-to-video/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/minimax/hailuo-02/pro/image-to-video)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                      "file": "hailou2.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "176",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "MiniMax Hailuo 02 [Pro] (Image to Video)",
                      "file": "hailou2.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.32",
          "name": "imagen4.md",
          "path": "02. Models/imagen4.md",
          "sections": [
            {
              "id": "177",
              "type": "section",
              "title": "Imagen 4",
              "content": "\n> Google’s highest quality image generation model\n\n",
              "file": "imagen4.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "178",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/imagen4/preview`\n- **Model ID**: `fal-ai/imagen4/preview`\n- **Category**: text-to-image\n- **Kind**: inference\n\n",
                  "parent_section": "Imagen 4",
                  "file": "imagen4.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "179",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Imagen 4",
                  "file": "imagen4.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "180",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The text prompt describing what you want to see\n  - Examples: \"Capture an intimate close-up bathed in warm, soft, late-afternoon sunlight filtering into a quintessential 1960s kitchen. The focal point is a charmingly designed vintage package of all-purpose flour, resting invitingly on a speckled Formica countertop. The packaging itself evokes pure nostalgia: perhaps thick, slightly textured paper in a warm cream tone, adorned with simple, bold typography (a friendly serif or script) in classic red and blue “ALL-PURPOSE FLOUR”, featuring a delightful illustration like a stylized sheaf of wheat or a cheerful baker character. In smaller bold print at the bottom of the package: “NET WT 5 LBS (80 OZ) 2.27kg”. Focus sharply on the package details – the slightly soft edges of the paper bag, the texture of the vintage printing, the inviting \\\"All-Purpose Flour\\\" text. Subtle hints of the 1960s kitchen frame the shot – the chrome edge of the counter gleaming softly, a blurred glimpse of a pastel yellow ceramic tile backsplash, or the corner of a vintage metal canister set just out of focus. The shallow depth of field keeps attention locked on the beautifully designed package, creating an aesthetic rich in warmth, authenticity, and nostalgic appeal.\"\n\n- **`negative_prompt`** (`string`, _optional_):\n  A description of what to discourage in the generated images Default value: `\"\"`\n  - Default: `\"\"`\n\n- **`aspect_ratio`** (`AspectRatioEnum`, _optional_):\n  The aspect ratio of the generated image Default value: `\"1:1\"`\n  - Default: `\"1:1\"`\n  - Options: `\"1:1\"`, `\"16:9\"`, `\"9:16\"`, `\"3:4\"`, `\"4:3\"`\n\n- **`num_images`** (`integer`, _optional_):\n  Number of images to generate (1-4) Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`seed`** (`integer`, _optional_):\n  Random seed for reproducible generation\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Capture an intimate close-up bathed in warm, soft, late-afternoon sunlight filtering into a quintessential 1960s kitchen. The focal point is a charmingly designed vintage package of all-purpose flour, resting invitingly on a speckled Formica countertop. The packaging itself evokes pure nostalgia: perhaps thick, slightly textured paper in a warm cream tone, adorned with simple, bold typography (a friendly serif or script) in classic red and blue “ALL-PURPOSE FLOUR”, featuring a delightful illustration like a stylized sheaf of wheat or a cheerful baker character. In smaller bold print at the bottom of the package: “NET WT 5 LBS (80 OZ) 2.27kg”. Focus sharply on the package details – the slightly soft edges of the paper bag, the texture of the vintage printing, the inviting \\\"All-Purpose Flour\\\" text. Subtle hints of the 1960s kitchen frame the shot – the chrome edge of the counter gleaming softly, a blurred glimpse of a pastel yellow ceramic tile backsplash, or the corner of a vintage metal canister set just out of focus. The shallow depth of field keeps attention locked on the beautifully designed package, creating an aesthetic rich in warmth, authenticity, and nostalgic appeal.\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Capture an intimate close-up bathed in warm, soft, late-afternoon sunlight filtering into a quintessential 1960s kitchen. The focal point is a charmingly designed vintage package of all-purpose flour, resting invitingly on a speckled Formica countertop. The packaging itself evokes pure nostalgia: perhaps thick, slightly textured paper in a warm cream tone, adorned with simple, bold typography (a friendly serif or script) in classic red and blue “ALL-PURPOSE FLOUR”, featuring a delightful illustration like a stylized sheaf of wheat or a cheerful baker character. In smaller bold print at the bottom of the package: “NET WT 5 LBS (80 OZ) 2.27kg”. Focus sharply on the package details – the slightly soft edges of the paper bag, the texture of the vintage printing, the inviting \\\"All-Purpose Flour\\\" text. Subtle hints of the 1960s kitchen frame the shot – the chrome edge of the counter gleaming softly, a blurred glimpse of a pastel yellow ceramic tile backsplash, or the corner of a vintage metal canister set just out of focus. The shallow depth of field keeps attention locked on the beautifully designed package, creating an aesthetic rich in warmth, authenticity, and nostalgic appeal.\",\n  \"aspect_ratio\": \"1:1\",\n  \"num_images\": 1\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Imagen 4",
                      "file": "imagen4.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "181",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Imagen 4",
                  "file": "imagen4.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "182",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/imagen4/preview \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Capture an intimate close-up bathed in warm, soft, late-afternoon sunlight filtering into a quintessential 1960s kitchen. The focal point is a charmingly designed vintage package of all-purpose flour, resting invitingly on a speckled Formica countertop. The packaging itself evokes pure nostalgia: perhaps thick, slightly textured paper in a warm cream tone, adorned with simple, bold typography (a friendly serif or script) in classic red and blue “ALL-PURPOSE FLOUR”, featuring a delightful illustration like a stylized sheaf of wheat or a cheerful baker character. In smaller bold print at the bottom of the package: “NET WT 5 LBS (80 OZ) 2.27kg”. Focus sharply on the package details – the slightly soft edges of the paper bag, the texture of the vintage printing, the inviting \\\"All-Purpose Flour\\\" text. Subtle hints of the 1960s kitchen frame the shot – the chrome edge of the counter gleaming softly, a blurred glimpse of a pastel yellow ceramic tile backsplash, or the corner of a vintage metal canister set just out of focus. The shallow depth of field keeps attention locked on the beautifully designed package, creating an aesthetic rich in warmth, authenticity, and nostalgic appeal.\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Imagen 4",
                      "file": "imagen4.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "183",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/imagen4/preview\",\n    arguments={\n        \"prompt\": \"Capture an intimate close-up bathed in warm, soft, late-afternoon sunlight filtering into a quintessential 1960s kitchen. The focal point is a charmingly designed vintage package of all-purpose flour, resting invitingly on a speckled Formica countertop. The packaging itself evokes pure nostalgia: perhaps thick, slightly textured paper in a warm cream tone, adorned with simple, bold typography (a friendly serif or script) in classic red and blue “ALL-PURPOSE FLOUR”, featuring a delightful illustration like a stylized sheaf of wheat or a cheerful baker character. In smaller bold print at the bottom of the package: “NET WT 5 LBS (80 OZ) 2.27kg”. Focus sharply on the package details – the slightly soft edges of the paper bag, the texture of the vintage printing, the inviting \\\"All-Purpose Flour\\\" text. Subtle hints of the 1960s kitchen frame the shot – the chrome edge of the counter gleaming softly, a blurred glimpse of a pastel yellow ceramic tile backsplash, or the corner of a vintage metal canister set just out of focus. The shallow depth of field keeps attention locked on the beautifully designed package, creating an aesthetic rich in warmth, authenticity, and nostalgic appeal.\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Imagen 4",
                      "file": "imagen4.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "184",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Imagen 4",
                  "file": "imagen4.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "185",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/imagen4/preview)\n- [API Documentation](https://fal.ai/models/fal-ai/imagen4/preview/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/imagen4/preview)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Imagen 4",
                      "file": "imagen4.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "186",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Imagen 4",
                      "file": "imagen4.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.33",
          "name": "kling2.1.md",
          "path": "02. Models/kling2.1.md",
          "sections": [
            {
              "id": "187",
              "type": "section",
              "title": "Kling 2.1 (pro)",
              "content": "\n> Kling 2.1 Pro is an advanced endpoint for the Kling 2.1 model, offering professional-grade videos with enhanced visual fidelity, precise camera movements, and dynamic motion control, perfect for cinematic storytelling.\n\n\n\n",
              "file": "kling2.1.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "188",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/kling-video/v2.1/pro/image-to-video`\n- **Model ID**: `fal-ai/kling-video/v2.1/pro/image-to-video`\n- **Category**: image-to-video\n- **Kind**: inference\n\n",
                  "parent_section": "Kling 2.1 (pro)",
                  "file": "kling2.1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "189",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Kling 2.1 (pro)",
                  "file": "kling2.1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "190",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_)\n  - Examples: \"Warm, incandescent streetlights paint the rain-slicked cobblestones in pools of amber light as a couple walks hand-in-hand, their silhouettes stark against the blurry backdrop of a city shrouded in a gentle downpour; the camera lingers on the subtle textures of their rain-soaked coats and the glistening reflections dancing on the wet pavement, creating a sense of intimate vulnerability and shared quietude.\"\n\n- **`image_url`** (`string`, _required_):\n  URL of the image to be used for the video\n  - Examples: \"https://v3.fal.media/files/lion/_I_io6Gtk83c72d-afXf8_image.webp\"\n\n- **`duration`** (`DurationEnum`, _optional_):\n  The duration of the generated video in seconds Default value: `\"5\"`\n  - Default: `\"5\"`\n  - Options: `\"5\"`, `\"10\"`\n\n- **`negative_prompt`** (`string`, _optional_):\n   Default value: `\"blur, distort, and low quality\"`\n  - Default: `\"blur, distort, and low quality\"`\n\n- **`cfg_scale`** (`float`, _optional_):\n  The CFG (Classifier Free Guidance) scale is a measure of how close you want\n  the model to stick to your prompt. Default value: `0.5`\n  - Default: `0.5`\n  - Range: `0` to `1`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Warm, incandescent streetlights paint the rain-slicked cobblestones in pools of amber light as a couple walks hand-in-hand, their silhouettes stark against the blurry backdrop of a city shrouded in a gentle downpour; the camera lingers on the subtle textures of their rain-soaked coats and the glistening reflections dancing on the wet pavement, creating a sense of intimate vulnerability and shared quietude.\",\n  \"image_url\": \"https://v3.fal.media/files/lion/_I_io6Gtk83c72d-afXf8_image.webp\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Warm, incandescent streetlights paint the rain-slicked cobblestones in pools of amber light as a couple walks hand-in-hand, their silhouettes stark against the blurry backdrop of a city shrouded in a gentle downpour; the camera lingers on the subtle textures of their rain-soaked coats and the glistening reflections dancing on the wet pavement, creating a sense of intimate vulnerability and shared quietude.\",\n  \"image_url\": \"https://v3.fal.media/files/lion/_I_io6Gtk83c72d-afXf8_image.webp\",\n  \"duration\": \"5\",\n  \"negative_prompt\": \"blur, distort, and low quality\",\n  \"cfg_scale\": 0.5\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Kling 2.1 (pro)",
                      "file": "kling2.1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "191",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Kling 2.1 (pro)",
                  "file": "kling2.1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "192",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/kling-video/v2.1/pro/image-to-video \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Warm, incandescent streetlights paint the rain-slicked cobblestones in pools of amber light as a couple walks hand-in-hand, their silhouettes stark against the blurry backdrop of a city shrouded in a gentle downpour; the camera lingers on the subtle textures of their rain-soaked coats and the glistening reflections dancing on the wet pavement, creating a sense of intimate vulnerability and shared quietude.\",\n     \"image_url\": \"https://v3.fal.media/files/lion/_I_io6Gtk83c72d-afXf8_image.webp\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Kling 2.1 (pro)",
                      "file": "kling2.1.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "193",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/kling-video/v2.1/pro/image-to-video\",\n    arguments={\n        \"prompt\": \"Warm, incandescent streetlights paint the rain-slicked cobblestones in pools of amber light as a couple walks hand-in-hand, their silhouettes stark against the blurry backdrop of a city shrouded in a gentle downpour; the camera lingers on the subtle textures of their rain-soaked coats and the glistening reflections dancing on the wet pavement, creating a sense of intimate vulnerability and shared quietude.\",\n        \"image_url\": \"https://v3.fal.media/files/lion/_I_io6Gtk83c72d-afXf8_image.webp\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Kling 2.1 (pro)",
                      "file": "kling2.1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "194",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Kling 2.1 (pro)",
                  "file": "kling2.1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "195",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/kling-video/v2.1/pro/image-to-video)\n- [API Documentation](https://fal.ai/models/fal-ai/kling-video/v2.1/pro/image-to-video/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/kling-video/v2.1/pro/image-to-video)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Kling 2.1 (pro)",
                      "file": "kling2.1.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "196",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Kling 2.1 (pro)",
                      "file": "kling2.1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.34",
          "name": "seedance1.md",
          "path": "02. Models/seedance1.md",
          "sections": [
            {
              "id": "197",
              "type": "section",
              "title": "Seedance 1.0 Pro",
              "content": "\n> Seedance 1.0 Pro, a high quality video generation model developed by Bytedance.\n\n",
              "file": "seedance1.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "198",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/bytedance/seedance/v1/pro/image-to-video`\n- **Model ID**: `fal-ai/bytedance/seedance/v1/pro/image-to-video`\n- **Category**: image-to-video\n- **Kind**: inference\n\n",
                  "parent_section": "Seedance 1.0 Pro",
                  "file": "seedance1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "199",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Seedance 1.0 Pro",
                  "file": "seedance1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "200",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The text prompt used to generate the video\n  - Examples: \"A skier glides over fresh snow, joyously smiling while kicking up large clouds of snow as he turns. Accelerating gradually down the slope, the camera moves smoothly alongside.\"\n\n- **`resolution`** (`ResolutionEnum`, _optional_):\n  Video resolution - 480p for faster generation, 1080p for higher quality Default value: `\"1080p\"`\n  - Default: `\"1080p\"`\n  - Options: `\"480p\"`, `\"1080p\"`\n\n- **`duration`** (`DurationEnum`, _optional_):\n  Duration of the video in seconds Default value: `\"5\"`\n  - Default: `\"5\"`\n  - Options: `\"5\"`, `\"10\"`\n\n- **`camera_fixed`** (`boolean`, _optional_):\n  Whether to fix the camera position\n  - Default: `false`\n\n- **`seed`** (`integer`, _optional_):\n  Random seed to control video generation. Use -1 for random.\n\n- **`image_url`** (`string`, _required_):\n  The URL of the image used to generate video\n  - Examples: \"https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v_img.jpg\"\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"A skier glides over fresh snow, joyously smiling while kicking up large clouds of snow as he turns. Accelerating gradually down the slope, the camera moves smoothly alongside.\",\n  \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v_img.jpg\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"A skier glides over fresh snow, joyously smiling while kicking up large clouds of snow as he turns. Accelerating gradually down the slope, the camera moves smoothly alongside.\",\n  \"resolution\": \"1080p\",\n  \"duration\": \"5\",\n  \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v_img.jpg\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Seedance 1.0 Pro",
                      "file": "seedance1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "201",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Seedance 1.0 Pro",
                  "file": "seedance1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "202",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/bytedance/seedance/v1/pro/image-to-video \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"A skier glides over fresh snow, joyously smiling while kicking up large clouds of snow as he turns. Accelerating gradually down the slope, the camera moves smoothly alongside.\",\n     \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v_img.jpg\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Seedance 1.0 Pro",
                      "file": "seedance1.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "203",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/bytedance/seedance/v1/pro/image-to-video\",\n    arguments={\n        \"prompt\": \"A skier glides over fresh snow, joyously smiling while kicking up large clouds of snow as he turns. Accelerating gradually down the slope, the camera moves smoothly alongside.\",\n        \"image_url\": \"https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v_img.jpg\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Seedance 1.0 Pro",
                      "file": "seedance1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "204",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Seedance 1.0 Pro",
                  "file": "seedance1.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "205",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/bytedance/seedance/v1/pro/image-to-video)\n- [API Documentation](https://fal.ai/models/fal-ai/bytedance/seedance/v1/pro/image-to-video/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/bytedance/seedance/v1/pro/image-to-video)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Seedance 1.0 Pro",
                      "file": "seedance1.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "206",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Seedance 1.0 Pro",
                      "file": "seedance1.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.35",
          "name": "seedance1lite.md",
          "path": "02. Models/seedance1lite.md",
          "sections": [
            {
              "id": "207",
              "type": "section",
              "title": "Seedance 1.0 Lite",
              "content": "\n> Seedance 1.0 Lite\n\n",
              "file": "seedance1lite.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "208",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/bytedance/seedance/v1/lite/image-to-video`\n- **Model ID**: `fal-ai/bytedance/seedance/v1/lite/image-to-video`\n- **Category**: image-to-video\n- **Kind**: inference\n\n",
                  "parent_section": "Seedance 1.0 Lite",
                  "file": "seedance1lite.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "209",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Seedance 1.0 Lite",
                  "file": "seedance1lite.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "210",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The text prompt used to generate the video\n  - Examples: \"A little dog is running in the sunshine. The camera follows the dog as it plays in a garden.\"\n\n- **`resolution`** (`ResolutionEnum`, _optional_):\n  Video resolution - 480p for faster generation, 720p for higher quality Default value: `\"720p\"`\n  - Default: `\"720p\"`\n  - Options: `\"480p\"`, `\"720p\"`, `\"1080p\"`\n\n- **`duration`** (`DurationEnum`, _optional_):\n  Duration of the video in seconds Default value: `\"5\"`\n  - Default: `\"5\"`\n  - Options: `\"5\"`, `\"10\"`\n\n- **`camera_fixed`** (`boolean`, _optional_):\n  Whether to fix the camera position\n  - Default: `false`\n\n- **`seed`** (`integer`, _optional_):\n  Random seed to control video generation. Use -1 for random.\n\n- **`image_url`** (`string`, _required_):\n  The URL of the image used to generate video\n  - Examples: \"https://fal.media/files/koala/f_xmiodPjhiKjdBkFmTu1.png\"\n\n- **`end_image_url`** (`string`, _optional_):\n  The URL of the image the video ends with. Defaults to None.\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"A little dog is running in the sunshine. The camera follows the dog as it plays in a garden.\",\n  \"image_url\": \"https://fal.media/files/koala/f_xmiodPjhiKjdBkFmTu1.png\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"A little dog is running in the sunshine. The camera follows the dog as it plays in a garden.\",\n  \"resolution\": \"720p\",\n  \"duration\": \"5\",\n  \"image_url\": \"https://fal.media/files/koala/f_xmiodPjhiKjdBkFmTu1.png\"\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Seedance 1.0 Lite",
                      "file": "seedance1lite.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "211",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Seedance 1.0 Lite",
                  "file": "seedance1lite.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "212",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/bytedance/seedance/v1/lite/image-to-video \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"A little dog is running in the sunshine. The camera follows the dog as it plays in a garden.\",\n     \"image_url\": \"https://fal.media/files/koala/f_xmiodPjhiKjdBkFmTu1.png\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Seedance 1.0 Lite",
                      "file": "seedance1lite.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "213",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/bytedance/seedance/v1/lite/image-to-video\",\n    arguments={\n        \"prompt\": \"A little dog is running in the sunshine. The camera follows the dog as it plays in a garden.\",\n        \"image_url\": \"https://fal.media/files/koala/f_xmiodPjhiKjdBkFmTu1.png\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Seedance 1.0 Lite",
                      "file": "seedance1lite.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "214",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Seedance 1.0 Lite",
                  "file": "seedance1lite.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "215",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/bytedance/seedance/v1/lite/image-to-video)\n- [API Documentation](https://fal.ai/models/fal-ai/bytedance/seedance/v1/lite/image-to-video/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/bytedance/seedance/v1/lite/image-to-video)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Seedance 1.0 Lite",
                      "file": "seedance1lite.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "216",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Seedance 1.0 Lite",
                      "file": "seedance1lite.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.36",
          "name": "seedream3.md",
          "path": "02. Models/seedream3.md",
          "sections": [
            {
              "id": "217",
              "type": "section",
              "title": "Bytedance",
              "content": "\n> Seedream 3.0 is a bilingual (Chinese and English) text-to-image model that excels at text-to-image generation.\n\n",
              "file": "seedream3.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "218",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/bytedance/seedream/v3/text-to-image`\n- **Model ID**: `fal-ai/bytedance/seedream/v3/text-to-image`\n- **Category**: text-to-image\n- **Kind**: inference\n\n",
                  "parent_section": "Bytedance",
                  "file": "seedream3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "219",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Bytedance",
                  "file": "seedream3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "220",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The text prompt used to generate the image\n  - Examples: \"Fisheye lens, the head of a cat, the image shows the effect that the facial features of the cat are distorted due to the shooting method.\"\n\n- **`image_size`** (`ImageSize | Enum`, _optional_):\n  Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048.\n  - One of: ImageSize | Enum\n\n- **`guidance_scale`** (`float`, _optional_):\n  Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `2.5`\n  - Default: `2.5`\n  - Range: `1` to `10`\n\n- **`num_images`** (`integer`, _optional_):\n  Number of images to generate Default value: `1`\n  - Default: `1`\n  - Range: `1` to `4`\n\n- **`seed`** (`integer`, _optional_):\n  Random seed to control the stochasticity of image generation.\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"Fisheye lens, the head of a cat, the image shows the effect that the facial features of the cat are distorted due to the shooting method.\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"Fisheye lens, the head of a cat, the image shows the effect that the facial features of the cat are distorted due to the shooting method.\",\n  \"guidance_scale\": 2.5,\n  \"num_images\": 1\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Bytedance",
                      "file": "seedream3.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "221",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Bytedance",
                  "file": "seedream3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "222",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/bytedance/seedream/v3/text-to-image \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"Fisheye lens, the head of a cat, the image shows the effect that the facial features of the cat are distorted due to the shooting method.\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Bytedance",
                      "file": "seedream3.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "223",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/bytedance/seedream/v3/text-to-image\",\n    arguments={\n        \"prompt\": \"Fisheye lens, the head of a cat, the image shows the effect that the facial features of the cat are distorted due to the shooting method.\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Bytedance",
                      "file": "seedream3.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "224",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Bytedance",
                  "file": "seedream3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "225",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/bytedance/seedream/v3/text-to-image)\n- [API Documentation](https://fal.ai/models/fal-ai/bytedance/seedream/v3/text-to-image/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/bytedance/seedream/v3/text-to-image)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Bytedance",
                      "file": "seedream3.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "226",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Bytedance",
                      "file": "seedream3.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.37",
          "name": "topaz video upscale.md",
          "path": "02. Models/topaz video upscale.md",
          "sections": [
            {
              "id": "227",
              "type": "section",
              "title": "Topaz Video Upscale",
              "content": "\n> Professional-grade video upscaling using Topaz technology. Enhance your videos with high-quality upscaling.\n\n",
              "file": "topaz video upscale.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "228",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/topaz/upscale/video`\n- **Model ID**: `fal-ai/topaz/upscale/video`\n- **Category**: video-to-video\n- **Kind**: inference\n**Tags**: upscaling, high-res\n\n\n",
                  "parent_section": "Topaz Video Upscale",
                  "file": "topaz video upscale.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "229",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Topaz Video Upscale",
                  "file": "topaz video upscale.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "230",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`video_url`** (`string`, _required_):\n  URL of the video to upscale\n  - Examples: \"https://v3.fal.media/files/kangaroo/y5-1YTGpun17eSeggZMzX_video-1733468228.mp4\"\n\n- **`upscale_factor`** (`float`, _optional_):\n  Factor to upscale the video by (e.g. 2.0 doubles width and height) Default value: `2`\n  - Default: `2`\n  - Range: `1` to `4`\n\n- **`target_fps`** (`integer`, _optional_):\n  Target FPS for frame interpolation. If set, frame interpolation will be enabled.\n  - Range: `16` to `60`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"video_url\": \"https://v3.fal.media/files/kangaroo/y5-1YTGpun17eSeggZMzX_video-1733468228.mp4\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"video_url\": \"https://v3.fal.media/files/kangaroo/y5-1YTGpun17eSeggZMzX_video-1733468228.mp4\",\n  \"upscale_factor\": 2\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Topaz Video Upscale",
                      "file": "topaz video upscale.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "231",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Topaz Video Upscale",
                  "file": "topaz video upscale.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "232",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/topaz/upscale/video \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"video_url\": \"https://v3.fal.media/files/kangaroo/y5-1YTGpun17eSeggZMzX_video-1733468228.mp4\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Topaz Video Upscale",
                      "file": "topaz video upscale.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "233",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/topaz/upscale/video\",\n    arguments={\n        \"video_url\": \"https://v3.fal.media/files/kangaroo/y5-1YTGpun17eSeggZMzX_video-1733468228.mp4\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Topaz Video Upscale",
                      "file": "topaz video upscale.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "234",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Topaz Video Upscale",
                  "file": "topaz video upscale.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "235",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/topaz/upscale/video)\n- [API Documentation](https://fal.ai/models/fal-ai/topaz/upscale/video/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/topaz/upscale/video)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Topaz Video Upscale",
                      "file": "topaz video upscale.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "236",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Topaz Video Upscale",
                      "file": "topaz video upscale.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "01.38",
          "name": "veo3.md",
          "path": "02. Models/veo3.md",
          "sections": [
            {
              "id": "237",
              "type": "section",
              "title": "Veo 3",
              "content": "\n> Veo 3 by Google, the most advanced AI video generation model in the world. With sound on!\n\n",
              "file": "veo3.md",
              "folder": "Fal Documentation",
              "subsections": [
                {
                  "id": "238",
                  "type": "subsection",
                  "title": "Overview",
                  "content": "\n- **Endpoint**: `https://fal.run/fal-ai/veo3`\n- **Model ID**: `fal-ai/veo3`\n- **Category**: text-to-video\n- **Kind**: inference\n\n",
                  "parent_section": "Veo 3",
                  "file": "veo3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": []
                },
                {
                  "id": "239",
                  "type": "subsection",
                  "title": "API Information",
                  "content": "\nThis model can be used via our HTTP API or more conveniently via our client libraries.\nSee the input and output schema below, as well as the usage examples.\n\n",
                  "parent_section": "Veo 3",
                  "file": "veo3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "240",
                      "type": "subsubsection",
                      "title": "Input Schema",
                      "content": "\nThe API accepts the following input parameters:\n\n\n- **`prompt`** (`string`, _required_):\n  The text prompt describing the video you want to generate\n  - Examples: \"A casual street interview on a busy New York City sidewalk in the afternoon. The interviewer holds a plain, unbranded microphone and asks: Have you seen Google's new Veo3 model It is a super good model. Person replies: Yeah I saw it, it's already available on fal. It's crazy good.\"\n\n- **`aspect_ratio`** (`AspectRatioEnum`, _optional_):\n  The aspect ratio of the generated video. If it is not set to 16:9, the video will be outpainted. Default value: `\"16:9\"`\n  - Default: `\"16:9\"`\n  - Options: `\"16:9\"`, `\"9:16\"`, `\"1:1\"`\n\n- **`duration`** (`DurationEnum`, _optional_):\n  The duration of the generated video in seconds Default value: `\"8s\"`\n  - Default: `\"8s\"`\n  - Options: `\"8s\"`\n\n- **`negative_prompt`** (`string`, _optional_):\n  A negative prompt to guide the video generation\n\n- **`enhance_prompt`** (`boolean`, _optional_):\n  Whether to enhance the video generation Default value: `true`\n  - Default: `true`\n\n- **`seed`** (`integer`, _optional_):\n  A seed to use for the video generation\n\n- **`resolution`** (`ResolutionEnum`, _optional_):\n  The resolution of the generated video Default value: `\"720p\"`\n  - Default: `\"720p\"`\n  - Options: `\"720p\"`, `\"1080p\"`\n\n- **`generate_audio`** (`boolean`, _optional_):\n  Whether to generate audio for the video. If false, %33 less credits will be used. Default value: `true`\n  - Default: `true`\n\n\n\n**Required Parameters Example**:\n\n```json\n{\n  \"prompt\": \"A casual street interview on a busy New York City sidewalk in the afternoon. The interviewer holds a plain, unbranded microphone and asks: Have you seen Google's new Veo3 model It is a super good model. Person replies: Yeah I saw it, it's already available on fal. It's crazy good.\"\n}\n```\n\n**Full Example**:\n\n```json\n{\n  \"prompt\": \"A casual street interview on a busy New York City sidewalk in the afternoon. The interviewer holds a plain, unbranded microphone and asks: Have you seen Google's new Veo3 model It is a super good model. Person replies: Yeah I saw it, it's already available on fal. It's crazy good.\",\n  \"aspect_ratio\": \"16:9\",\n  \"duration\": \"8s\",\n  \"enhance_prompt\": true,\n  \"resolution\": \"720p\",\n  \"generate_audio\": true\n}\n```\n\n",
                      "parent_subsection": "API Information",
                      "parent_section": "Veo 3",
                      "file": "veo3.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "241",
                  "type": "subsection",
                  "title": "Usage Examples",
                  "content": "",
                  "parent_section": "Veo 3",
                  "file": "veo3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "242",
                      "type": "subsubsection",
                      "title": "cURL",
                      "content": "\n```bash\ncurl --request POST \\\n  --url https://fal.run/fal-ai/veo3 \\\n  --header \"Authorization: Key $FAL_KEY\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n     \"prompt\": \"A casual street interview on a busy New York City sidewalk in the afternoon. The interviewer holds a plain, unbranded microphone and asks: Have you seen Google's new Veo3 model It is a super good model. Person replies: Yeah I saw it, it's already available on fal. It's crazy good.\"\n   }'\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Veo 3",
                      "file": "veo3.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "243",
                      "type": "subsubsection",
                      "title": "Python",
                      "content": "\nEnsure you have the Python client installed:\n\n```bash\npip install fal-client\n```\n\nThen use the API client to make requests:\n\n```python\nimport fal_client\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log[\"message\"])\n\nresult = fal_client.subscribe(\n    \"fal-ai/veo3\",\n    arguments={\n        \"prompt\": \"A casual street interview on a busy New York City sidewalk in the afternoon. The interviewer holds a plain, unbranded microphone and asks: Have you seen Google's new Veo3 model It is a super good model. Person replies: Yeah I saw it, it's already available on fal. It's crazy good.\"\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n```\n",
                      "parent_subsection": "Usage Examples",
                      "parent_section": "Veo 3",
                      "file": "veo3.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                },
                {
                  "id": "244",
                  "type": "subsection",
                  "title": "Additional Resources",
                  "content": "",
                  "parent_section": "Veo 3",
                  "file": "veo3.md",
                  "folder": "Fal Documentation",
                  "subsubsections": [
                    {
                      "id": "245",
                      "type": "subsubsection",
                      "title": "Documentation",
                      "content": "\n- [Model Playground](https://fal.ai/models/fal-ai/veo3)\n- [API Documentation](https://fal.ai/models/fal-ai/veo3/api)\n- [OpenAPI Schema](https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/veo3)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Veo 3",
                      "file": "veo3.md",
                      "folder": "Fal Documentation"
                    },
                    {
                      "id": "246",
                      "type": "subsubsection",
                      "title": "fal.ai Platform",
                      "content": "\n- [Platform Documentation](https://docs.fal.ai)\n- [Python Client](https://docs.fal.ai/clients/python)\n- [JavaScript Client](https://docs.fal.ai/clients/javascript)\n",
                      "parent_subsection": "Additional Resources",
                      "parent_section": "Veo 3",
                      "file": "veo3.md",
                      "folder": "Fal Documentation"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}